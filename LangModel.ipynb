{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\kerts\\documents\\projects\\python\\keyboardswitch\\venv\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: Levenshtein==0.21.1 in c:\\users\\kerts\\documents\\projects\\python\\keyboardswitch\\venv\\lib\\site-packages (from python-Levenshtein) (0.21.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\kerts\\documents\\projects\\python\\keyboardswitch\\venv\\lib\\site-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:02.774696500Z",
     "start_time": "2023-09-15T10:22:58.754697600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.utils.path import get_long_path_name # TBD\n",
    "import Levenshtein"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T07:35:01.651252800Z",
     "start_time": "2023-09-16T07:35:00.640259100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper Functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load vocabulary into the set from file\n",
    "def get_language_words_set(file_path, encoding='utf-8'):\n",
    "\n",
    "    words_set = set()\n",
    "    try:\n",
    "        #with open(file_path, 'r', encoding='cp1251') as file:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            for line in file:\n",
    "                word = line.strip().lower()  # Remove leading/trailing whitespaces and convert to lowercase\n",
    "                words_set.add(word)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return words_set"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T07:35:01.663249900Z",
     "start_time": "2023-09-16T07:35:01.656250300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Save vocabulary into the file\n",
    "def save_words_to_file(words_set, file_path, encoding='utf-8'):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding=encoding) as file:\n",
    "            for word in words_set:\n",
    "                file.write(word + '\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T07:08:06.193631700Z",
     "start_time": "2023-09-16T07:08:06.159632200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Check, if word exists in vocabulary\n",
    "def is_in_set(words_set, word):\n",
    "    return word in words_set"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T07:08:10.155902400Z",
     "start_time": "2023-09-16T07:08:10.107904300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def jaccard_similarity(str1, str2):\n",
    "    set1 = set(str1)\n",
    "    set2 = set(str2)\n",
    "\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "\n",
    "    similarity = intersection / union\n",
    "    return similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:03.723697Z",
     "start_time": "2023-09-15T10:23:03.663698Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def ngram_similarity(str1, str2, n):\n",
    "    def get_ngrams(s, n):\n",
    "        return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "    ngrams1 = get_ngrams(str1, n)\n",
    "    ngrams2 = get_ngrams(str2, n)\n",
    "\n",
    "    intersection = len(set(ngrams1).intersection(ngrams2))\n",
    "    union = len(set(ngrams1).union(ngrams2))\n",
    "\n",
    "    similarity = intersection / union\n",
    "    return similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:03.771700300Z",
     "start_time": "2023-09-15T10:23:03.695709400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load the language sets:\n",
    "english_vocabulary = get_language_words_set('vocabulary/english_vocabulary')\n",
    "hebrew_vocabulary = get_language_words_set('vocabulary/hebrew_vocabulary')\n",
    "russian_vocabulary = get_language_words_set('vocabulary/russian_vocabulary')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T07:35:07.205091900Z",
     "start_time": "2023-09-16T07:35:04.732092400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Make the layouts\n",
    "russian_layout = 'ёйцукенгшщзхъфывапролджэ\\ячсмитьбю.ЁЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭ/ЯЧСМИТЬБЮ,'\n",
    "english_layout = '''`qwertyuiop[]asdfghjkl;'\\zxcvbnm,./~QWERTYUIOP{}ASDFGHJKL:\"|ZXCVBNM<>?'''\n",
    "hebrew_layout = \";/'קראטוןםפ][שדגכעיחלךף,\\זסבהנמצתץ.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T20:08:45.368824Z",
     "start_time": "2023-09-16T20:08:45.361826200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Creating conversion maps per characted position\n",
    "def create_conversion_map(source_layout, target_layout):\n",
    "    conversion_map = {}\n",
    "\n",
    "    for src_char, tgt_char in zip(source_layout, target_layout):\n",
    "        conversion_map[ord(src_char)] = ord(tgt_char)\n",
    "    return conversion_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.574698700Z",
     "start_time": "2023-09-15T10:23:06.548698200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Converting text from one map to another\n",
    "def convert_text(text, conversion_map):\n",
    "    return text.translate(conversion_map)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.574698700Z",
     "start_time": "2023-09-15T10:23:06.557700100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Making the conversion\n",
    "def convert_text_bidirectional(text, from_layout, to_layout):\n",
    "    if to_layout == hebrew_layout:\n",
    "        text = text.lower()\n",
    "    return convert_text(text, create_conversion_map(from_layout, to_layout))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.612697700Z",
     "start_time": "2023-09-15T10:23:06.573701200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_language(word):\n",
    "    \"\"\"\n",
    "    en = 1\n",
    "    he = 2\n",
    "    ru = 3\n",
    "    NA = 0\n",
    "    \"\"\"\n",
    "    lang = [is_in_set(english_vocabulary,word),is_in_set(hebrew_vocabulary,word),is_in_set(russian_vocabulary,word)]\n",
    "    if lang.count(True) == 0 or lang.count(True)>1:\n",
    "        return 0\n",
    "    else:\n",
    "        for index, value in enumerate(lang):\n",
    "            if value:\n",
    "                return index+1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.841698400Z",
     "start_time": "2023-09-15T10:23:06.593703500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Generate the maps\n",
    "russian_to_english_map = create_conversion_map(russian_layout, english_layout)\n",
    "russian_to_hebrew_map = create_conversion_map(russian_layout, hebrew_layout)\n",
    "english_to_russian_map = create_conversion_map(english_layout, russian_layout)\n",
    "english_to_hebrew_map = create_conversion_map(english_layout, hebrew_layout)\n",
    "hebrew_to_russian_map = create_conversion_map(hebrew_layout, russian_layout)\n",
    "hebrew_to_english_map = create_conversion_map(hebrew_layout, english_layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.894700900Z",
     "start_time": "2023-09-15T10:23:06.851699100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "some examples to use the helpers:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_in_set(english_vocabulary,'hellow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:06.938701700Z",
     "start_time": "2023-09-15T10:23:06.879700300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Levenshtein distance between 'hello' and 'hello1' is 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "str1 = \"hello\"\n",
    "str2 = \"hello1\"\n",
    "\n",
    "lev_distance = Levenshtein.distance(str1, str2)/max(len(str1),len(str2))\n",
    "print(f\"Normalized Levenshtein distance between '{str1}' and '{str2}' is {lev_distance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:07.023702300Z",
     "start_time": "2023-09-15T10:23:06.915701200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram similarity between 'hello' and 'hello': 1.0\n"
     ]
    }
   ],
   "source": [
    "string1 = \"hello\"\n",
    "string2 = \"hello\"\n",
    "n_value = 2\n",
    "\n",
    "similarity = ngram_similarity(string1, string2, n_value)\n",
    "print(f\"N-gram similarity between '{string1}' and '{string2}': {similarity}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:07.026701700Z",
     "start_time": "2023-09-15T10:23:06.940702100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'ghbdtn'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_text_bidirectional('привет',russian_layout,english_layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:07.068699400Z",
     "start_time": "2023-09-15T10:23:06.955700500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "General ideas.\n",
    "We need to generate the train/test sets from the vocabularies, together with the dummy words and consequently train the model on dataset n-grams.\n",
    "The model will learn to predict the correct language set.\n",
    "short n-grams will probably have a bad accuracy (precision,recall,f1), therefore we will need to understand the correct minimal length for the good prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, lets create the ballanced dataset, where the words will be taken from the real vocabulary and the dummies will be converted by transorming the encoding to other forms\n",
    "en-word : he->en word\n",
    "en-word : ru->en word\n",
    "he word : en->he word\n",
    "he word : ru->he word\n",
    "ru word : en->ru word\n",
    "ru word : he->ru word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the english vocabulary contains 466562 words\n",
      "the russian vocabulary contains 1528910 words\n",
      "the hebrew vocabulary contains 469509 words\n"
     ]
    }
   ],
   "source": [
    "print(f'the english vocabulary contains {len(english_vocabulary)} words')\n",
    "print(f'the russian vocabulary contains {len(russian_vocabulary)} words')\n",
    "print(f'the hebrew vocabulary contains {len(hebrew_vocabulary)} words')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:07.070699900Z",
     "start_time": "2023-09-15T10:23:06.986697800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "english and hebrew vocabularies are almost identical in length. russian is 3 times bigger. we will need to take that into account.\n",
    "May be the dataset shall contain all the words from all the datasets with 3 different layouts as a data for feature extraction and a label as target?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# make pandas dataset\n",
    "def makeDataset():\n",
    "    dataset = pd.DataFrame(columns=['en', 'he', 'ru','target'])\n",
    "\n",
    "\n",
    "    rset = pd.DataFrame([{'en': convert_text_bidirectional(word,russian_layout,english_layout),\n",
    "                        'he': convert_text_bidirectional(word,russian_layout,hebrew_layout),\n",
    "                        'ru': word,\n",
    "                        'target':get_language(word)}\n",
    "                         for word in russian_vocabulary])\n",
    "\n",
    "\n",
    "    eset = pd.DataFrame([{'en': word,\n",
    "                        'he': convert_text_bidirectional(word,english_layout,hebrew_layout),\n",
    "                        'ru': convert_text_bidirectional(word,english_layout,russian_layout),\n",
    "                        'target':get_language(word)}\n",
    "                         for word in english_vocabulary])\n",
    "\n",
    "    hset = pd.DataFrame([{'en': convert_text_bidirectional(word,hebrew_layout,english_layout),\n",
    "                        'he': word,\n",
    "                        'ru': convert_text_bidirectional(word,hebrew_layout,russian_layout),\n",
    "                        'target':get_language(word)}\n",
    "                         for word in hebrew_vocabulary])\n",
    "\n",
    "    dataset = pd.concat([dataset,rset,eset,hset],ignore_index=True)\n",
    "\n",
    "    # Shuffle the DataFrame\n",
    "    #dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:23:07.245696600Z",
     "start_time": "2023-09-15T10:23:07.009700100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df = makeDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:41:47.018624900Z",
     "start_time": "2023-09-15T10:40:29.820067800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                       en                he                ru target\n0            ,fhf,fyobwt.      תכיכתכטםנ'אץ      барабанщицею      3\n1        fdnjgjl]`vybrfvb  כגמחעחך[;הטנרכהנ  автоподъёмниками      3\n2             pfneitdfkcz       פכמקןאגכלבז       затушевался      3\n3             ibhjrjkj,s[       ןניחרחלחתד]       широколобых      3\n4              ghjvjrfire        עיחהחרכןרק        промокашку      3\n...                   ...               ...               ...    ...\n2464976             vurhi             הורין             мгкрш      2\n2464977             fshvo             כדיהם             аырмщ      2\n2464978           dhjfubu           גיחכונו           вроагиг      2\n2464979            nuskfi            מודלכן            тгылаш      2\n2464980              ,dve              תגהק              бвму      2\n\n[2464981 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>he</th>\n      <th>ru</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>,fhf,fyobwt.</td>\n      <td>תכיכתכטםנ'אץ</td>\n      <td>барабанщицею</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fdnjgjl]`vybrfvb</td>\n      <td>כגמחעחך[;הטנרכהנ</td>\n      <td>автоподъёмниками</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pfneitdfkcz</td>\n      <td>פכמקןאגכלבז</td>\n      <td>затушевался</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ibhjrjkj,s[</td>\n      <td>ןניחרחלחתד]</td>\n      <td>широколобых</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ghjvjrfire</td>\n      <td>עיחהחרכןרק</td>\n      <td>промокашку</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2464976</th>\n      <td>vurhi</td>\n      <td>הורין</td>\n      <td>мгкрш</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2464977</th>\n      <td>fshvo</td>\n      <td>כדיהם</td>\n      <td>аырмщ</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2464978</th>\n      <td>dhjfubu</td>\n      <td>גיחכונו</td>\n      <td>вроагиг</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2464979</th>\n      <td>nuskfi</td>\n      <td>מודלכן</td>\n      <td>тгылаш</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2464980</th>\n      <td>,dve</td>\n      <td>תגהק</td>\n      <td>бвму</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>2464981 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T10:41:47.061628400Z",
     "start_time": "2023-09-15T10:41:47.022623800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Create a new DataFrame for the entire dataset\n",
    "def map_target(row):\n",
    "    def lang(x):\n",
    "        if x == 1:\n",
    "            return 'en'\n",
    "        if x == 2:\n",
    "            return 'he'\n",
    "        if x == 3:\n",
    "            return 'ru'\n",
    "\n",
    "    new_rows = []\n",
    "    for i in range(1, 4):\n",
    "        new_row = {'word': row[lang(i)], 'target': i if row['target'] == i else 0}\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "new_df = df.apply(lambda row: map_target(row), axis=1)\n",
    "\n",
    "# Reshape the resulting DataFrame\n",
    "new_df = pd.concat(new_df.tolist(), ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:08.040765800Z",
     "start_time": "2023-09-15T11:53:47.921561500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "shuffled_df.to_pickle('df_data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T12:15:40.343972Z",
     "start_time": "2023-09-15T12:15:35.307630500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "shuffled_df.to_csv('df_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T12:16:14.536965500Z",
     "start_time": "2023-09-15T12:16:03.281044800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Shuffle the rows\n",
    "shuffled_df = new_df.sample(frac=1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:59.064220200Z",
     "start_time": "2023-09-15T12:14:56.764217300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Steps towards the model (it shall be a multi-layer LSTM)\n",
    "--------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:08:21.508955500Z",
     "start_time": "2023-09-16T21:08:21.027956800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the DataFrame from a pickle file\n",
    "df = pd.read_pickle('df_data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:08:26.923561700Z",
     "start_time": "2023-09-16T21:08:24.321562200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Creating a combined symbol layout for 3 languages:\n",
    "# Make the layouts\n",
    "russian_layout = 'ёйцукенгшщзхъфывапролджэ\\ячсмитьбю.ЁЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭ/ЯЧСМИТЬБЮ,'\n",
    "english_layout = '''`qwertyuiop[]asdfghjkl;'\\zxcvbnm,./~QWERTYUIOP{}ASDFGHJKL:\"|ZXCVBNM<>?'''\n",
    "hebrew_layout = \";/'קראטוןםפ][שדגכעיחלךף,\\זסבהנמצתץ.\"\n",
    "special_characters = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ \"\n",
    "numbers = \"0123456789\"\n",
    "combined_layout = english_layout+hebrew_layout+russian_layout+numbers+special_characters\n",
    "unique_chars = set()\n",
    "\n",
    "for char in combined_layout:\n",
    "    if char not in unique_chars:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "layout = list(unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:08:26.934562900Z",
     "start_time": "2023-09-16T21:08:26.918560200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "char_to_index = {char: i for i, char in enumerate(layout)}\n",
    "index_to_char = {i: char for i, char in enumerate(layout)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:08:30.216186900Z",
     "start_time": "2023-09-16T21:08:30.159186400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dictionary.pkl', 'wb') as file:\n",
    "    pickle.dump(char_to_index, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T06:49:03.412Z",
     "start_time": "2023-09-17T06:49:03.384003500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1d56c692590>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T17:45:18.650480700Z",
     "start_time": "2023-09-17T17:45:13.145041900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:08:48.103340900Z",
     "start_time": "2023-09-16T21:08:48.035334500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T20:09:20.369009100Z",
     "start_time": "2023-09-16T20:09:20.357008400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Максимальная длина слова для паддинга\n",
    "max_length = max([len(word) for word in df['word']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:03.177590300Z",
     "start_time": "2023-09-16T21:09:01.483586300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Разделение датасета на train и test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:10.475338300Z",
     "start_time": "2023-09-16T21:09:07.776340200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_size = len(char_to_index)  # Размер словаря (количество уникальных символов)\n",
    "hidden_size = 64  # Размер скрытого состояния LSTM\n",
    "num_layers = 3  # Количество слоев LSTM\n",
    "num_classes = 4  # Количество классов (количество языков) включая 0 = нет языка"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:12.747614400Z",
     "start_time": "2023-09-16T21:09:12.733609400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "class LanguageClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LanguageClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Изменяем размерность входных данных\n",
    "        x = x.view(x.size(0), -1)  # Приводим к размерности (batch_size, sequence_length)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(self.embedding(x), (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T17:45:24.934226800Z",
     "start_time": "2023-09-17T17:45:24.897233200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Создаем кастомный Dataset для загрузки данных из датафрейма\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, char_to_index, max_length):\n",
    "        self.data = dataframe\n",
    "        self.char_to_index = char_to_index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.data.iloc[idx][\"word\"]\n",
    "        target = self.data.iloc[idx][\"target\"]\n",
    "\n",
    "        # Преобразование слова в индексы символов с ограничением до размера словаря\n",
    "        input_indices = [self.char_to_index.get(char, 0) for char in word if char in self.char_to_index]\n",
    "\n",
    "        # Добавление паддинга\n",
    "        if len(input_indices) < self.max_length:\n",
    "            num_padding = self.max_length - len(input_indices)\n",
    "            input_indices += [0] * num_padding  # 0 - индекс паддинга\n",
    "\n",
    "        # Преобразование в тензор PyTorch и изменение формы\n",
    "        input_tensor = torch.tensor(input_indices).view(1, -1)  # 1 - размер батча, -1 - автоматический расчет размерности\n",
    "\n",
    "        return input_tensor, target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:20.595514700Z",
     "start_time": "2023-09-16T21:09:20.574515500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Создаем кастомный Dataset\n",
    "# custom_dataset = CustomDataset(df, char_to_index, max_length)\n",
    "train_dataset = CustomDataset(train_df, char_to_index, max_length)\n",
    "test_dataset = CustomDataset(test_df, char_to_index, max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:23.842783Z",
     "start_time": "2023-09-16T21:09:23.832791700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Создаем DataLoader для обучения\n",
    "batch_size = 1024  # Размер батча\n",
    "# dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:26.623474Z",
     "start_time": "2023-09-16T21:09:26.598476700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Создание экземпляра модели\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m LanguageClassifier(\u001B[43minput_size\u001B[49m, hidden_size, num_layers, num_classes)\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Создание экземпляра модели\n",
    "model = LanguageClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T17:45:48.351050400Z",
     "start_time": "2023-09-17T17:45:47.415052Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:34.489644900Z",
     "start_time": "2023-09-16T21:09:34.478644900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return accuracy, average_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:46.185121200Z",
     "start_time": "2023-09-16T21:09:46.160127200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "patience = 3\n",
    "min_delta = 0.001  # Minimum improvement required to continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:52.939128200Z",
     "start_time": "2023-09-16T21:09:52.903128900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:15:57.447040200Z",
     "start_time": "2023-09-16T21:15:57.367041400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Batch [0/5778] Loss: 0.9452\n",
      "Batch [10/5778] Loss: 0.9665\n",
      "Batch [20/5778] Loss: 0.9579\n",
      "Batch [30/5778] Loss: 0.9776\n",
      "Batch [40/5778] Loss: 0.9112\n",
      "Batch [50/5778] Loss: 0.9261\n",
      "Batch [60/5778] Loss: 0.9132\n",
      "Batch [70/5778] Loss: 0.9361\n",
      "Batch [80/5778] Loss: 0.9572\n",
      "Batch [90/5778] Loss: 0.9729\n",
      "Batch [100/5778] Loss: 0.9371\n",
      "Batch [110/5778] Loss: 0.9426\n",
      "Batch [120/5778] Loss: 0.9366\n",
      "Batch [130/5778] Loss: 0.9413\n",
      "Batch [140/5778] Loss: 0.9490\n",
      "Batch [150/5778] Loss: 0.9235\n",
      "Batch [160/5778] Loss: 0.9416\n",
      "Batch [170/5778] Loss: 0.9686\n",
      "Batch [180/5778] Loss: 0.9568\n",
      "Batch [190/5778] Loss: 0.9646\n",
      "Batch [200/5778] Loss: 0.9442\n",
      "Batch [210/5778] Loss: 0.9153\n",
      "Batch [220/5778] Loss: 0.9404\n",
      "Batch [230/5778] Loss: 0.9170\n",
      "Batch [240/5778] Loss: 0.8915\n",
      "Batch [250/5778] Loss: 0.9583\n",
      "Batch [260/5778] Loss: 0.9284\n",
      "Batch [270/5778] Loss: 0.9992\n",
      "Batch [280/5778] Loss: 0.9665\n",
      "Batch [290/5778] Loss: 0.9410\n",
      "Batch [300/5778] Loss: 0.9500\n",
      "Batch [310/5778] Loss: 0.9199\n",
      "Batch [320/5778] Loss: 0.9344\n",
      "Batch [330/5778] Loss: 0.9344\n",
      "Batch [340/5778] Loss: 0.9762\n",
      "Batch [350/5778] Loss: 0.9293\n",
      "Batch [360/5778] Loss: 0.9412\n",
      "Batch [370/5778] Loss: 0.9546\n",
      "Batch [380/5778] Loss: 0.9495\n",
      "Batch [390/5778] Loss: 0.9316\n",
      "Batch [400/5778] Loss: 0.9487\n",
      "Batch [410/5778] Loss: 0.9104\n",
      "Batch [420/5778] Loss: 0.9650\n",
      "Batch [430/5778] Loss: 0.9603\n",
      "Batch [440/5778] Loss: 0.9093\n",
      "Batch [450/5778] Loss: 0.9660\n",
      "Batch [460/5778] Loss: 0.9511\n",
      "Batch [470/5778] Loss: 0.9541\n",
      "Batch [480/5778] Loss: 0.9749\n",
      "Batch [490/5778] Loss: 0.9162\n",
      "Batch [500/5778] Loss: 0.9522\n",
      "Batch [510/5778] Loss: 0.9388\n",
      "Batch [520/5778] Loss: 0.9799\n",
      "Batch [530/5778] Loss: 0.9075\n",
      "Batch [540/5778] Loss: 0.9714\n",
      "Batch [550/5778] Loss: 0.9573\n",
      "Batch [560/5778] Loss: 0.9600\n",
      "Batch [570/5778] Loss: 0.9473\n",
      "Batch [580/5778] Loss: 0.9523\n",
      "Batch [590/5778] Loss: 0.9778\n",
      "Batch [600/5778] Loss: 0.9371\n",
      "Batch [610/5778] Loss: 0.9314\n",
      "Batch [620/5778] Loss: 0.9957\n",
      "Batch [630/5778] Loss: 0.9797\n",
      "Batch [640/5778] Loss: 0.9273\n",
      "Batch [650/5778] Loss: 0.9427\n",
      "Batch [660/5778] Loss: 0.9727\n",
      "Batch [670/5778] Loss: 0.9472\n",
      "Batch [680/5778] Loss: 0.8993\n",
      "Batch [690/5778] Loss: 0.9523\n",
      "Batch [700/5778] Loss: 0.9399\n",
      "Batch [710/5778] Loss: 0.9206\n",
      "Batch [720/5778] Loss: 0.9562\n",
      "Batch [730/5778] Loss: 0.9768\n",
      "Batch [740/5778] Loss: 0.9065\n",
      "Batch [750/5778] Loss: 0.9377\n",
      "Batch [760/5778] Loss: 0.9024\n",
      "Batch [770/5778] Loss: 0.9395\n",
      "Batch [780/5778] Loss: 0.9501\n",
      "Batch [790/5778] Loss: 0.9206\n",
      "Batch [800/5778] Loss: 0.9537\n",
      "Batch [810/5778] Loss: 0.9291\n",
      "Batch [820/5778] Loss: 0.8960\n",
      "Batch [830/5778] Loss: 0.9471\n",
      "Batch [840/5778] Loss: 0.9138\n",
      "Batch [850/5778] Loss: 0.9529\n",
      "Batch [860/5778] Loss: 0.9771\n",
      "Batch [870/5778] Loss: 0.9435\n",
      "Batch [880/5778] Loss: 0.9520\n",
      "Batch [890/5778] Loss: 1.0035\n",
      "Batch [900/5778] Loss: 0.9001\n",
      "Batch [910/5778] Loss: 0.9250\n",
      "Batch [920/5778] Loss: 0.9375\n",
      "Batch [930/5778] Loss: 0.9200\n",
      "Batch [940/5778] Loss: 0.9776\n",
      "Batch [950/5778] Loss: 0.9311\n",
      "Batch [960/5778] Loss: 0.9260\n",
      "Batch [970/5778] Loss: 0.9355\n",
      "Batch [980/5778] Loss: 0.9202\n",
      "Batch [990/5778] Loss: 0.9715\n",
      "Batch [1000/5778] Loss: 0.8940\n",
      "Batch [1010/5778] Loss: 0.9615\n",
      "Batch [1020/5778] Loss: 0.9710\n",
      "Batch [1030/5778] Loss: 0.9841\n",
      "Batch [1040/5778] Loss: 0.9621\n",
      "Batch [1050/5778] Loss: 0.9019\n",
      "Batch [1060/5778] Loss: 0.9225\n",
      "Batch [1070/5778] Loss: 0.9597\n",
      "Batch [1080/5778] Loss: 0.9368\n",
      "Batch [1090/5778] Loss: 0.9440\n",
      "Batch [1100/5778] Loss: 0.9540\n",
      "Batch [1110/5778] Loss: 0.9356\n",
      "Batch [1120/5778] Loss: 0.8920\n",
      "Batch [1130/5778] Loss: 0.9586\n",
      "Batch [1140/5778] Loss: 0.9080\n",
      "Batch [1150/5778] Loss: 0.9705\n",
      "Batch [1160/5778] Loss: 0.9861\n",
      "Batch [1170/5778] Loss: 0.9273\n",
      "Batch [1180/5778] Loss: 0.9674\n",
      "Batch [1190/5778] Loss: 0.8934\n",
      "Batch [1200/5778] Loss: 0.9756\n",
      "Batch [1210/5778] Loss: 0.9274\n",
      "Batch [1220/5778] Loss: 0.9445\n",
      "Batch [1230/5778] Loss: 0.9586\n",
      "Batch [1240/5778] Loss: 0.9259\n",
      "Batch [1250/5778] Loss: 0.9447\n",
      "Batch [1260/5778] Loss: 0.9552\n",
      "Batch [1270/5778] Loss: 0.9359\n",
      "Batch [1280/5778] Loss: 0.9826\n",
      "Batch [1290/5778] Loss: 0.9476\n",
      "Batch [1300/5778] Loss: 0.9119\n",
      "Batch [1310/5778] Loss: 0.9453\n",
      "Batch [1320/5778] Loss: 0.9510\n",
      "Batch [1330/5778] Loss: 0.9302\n",
      "Batch [1340/5778] Loss: 0.9336\n",
      "Batch [1350/5778] Loss: 0.9656\n",
      "Batch [1360/5778] Loss: 0.9170\n",
      "Batch [1370/5778] Loss: 0.9312\n",
      "Batch [1380/5778] Loss: 0.9573\n",
      "Batch [1390/5778] Loss: 0.9624\n",
      "Batch [1400/5778] Loss: 0.9086\n",
      "Batch [1410/5778] Loss: 0.9256\n",
      "Batch [1420/5778] Loss: 0.9201\n",
      "Batch [1430/5778] Loss: 0.9783\n",
      "Batch [1440/5778] Loss: 0.9614\n",
      "Batch [1450/5778] Loss: 0.9652\n",
      "Batch [1460/5778] Loss: 0.8766\n",
      "Batch [1470/5778] Loss: 0.9722\n",
      "Batch [1480/5778] Loss: 0.9663\n",
      "Batch [1490/5778] Loss: 0.8904\n",
      "Batch [1500/5778] Loss: 0.9810\n",
      "Batch [1510/5778] Loss: 1.0196\n",
      "Batch [1520/5778] Loss: 0.9658\n",
      "Batch [1530/5778] Loss: 0.9193\n",
      "Batch [1540/5778] Loss: 0.9602\n",
      "Batch [1550/5778] Loss: 0.9643\n",
      "Batch [1560/5778] Loss: 0.9519\n",
      "Batch [1570/5778] Loss: 0.9367\n",
      "Batch [1580/5778] Loss: 0.9874\n",
      "Batch [1590/5778] Loss: 0.9084\n",
      "Batch [1600/5778] Loss: 0.9538\n",
      "Batch [1610/5778] Loss: 1.0003\n",
      "Batch [1620/5778] Loss: 0.9756\n",
      "Batch [1630/5778] Loss: 0.9306\n",
      "Batch [1640/5778] Loss: 0.9222\n",
      "Batch [1650/5778] Loss: 0.9667\n",
      "Batch [1660/5778] Loss: 0.9486\n",
      "Batch [1670/5778] Loss: 0.9503\n",
      "Batch [1680/5778] Loss: 0.9457\n",
      "Batch [1690/5778] Loss: 0.9118\n",
      "Batch [1700/5778] Loss: 0.9518\n",
      "Batch [1710/5778] Loss: 0.9578\n",
      "Batch [1720/5778] Loss: 0.9251\n",
      "Batch [1730/5778] Loss: 0.9302\n",
      "Batch [1740/5778] Loss: 0.9293\n",
      "Batch [1750/5778] Loss: 0.9184\n",
      "Batch [1760/5778] Loss: 0.9305\n",
      "Batch [1770/5778] Loss: 0.9388\n",
      "Batch [1780/5778] Loss: 0.9617\n",
      "Batch [1790/5778] Loss: 0.9076\n",
      "Batch [1800/5778] Loss: 0.9420\n",
      "Batch [1810/5778] Loss: 0.9157\n",
      "Batch [1820/5778] Loss: 0.9617\n",
      "Batch [1830/5778] Loss: 0.9313\n",
      "Batch [1840/5778] Loss: 0.9715\n",
      "Batch [1850/5778] Loss: 0.8967\n",
      "Batch [1860/5778] Loss: 0.9223\n",
      "Batch [1870/5778] Loss: 0.9441\n",
      "Batch [1880/5778] Loss: 1.0154\n",
      "Batch [1890/5778] Loss: 0.9445\n",
      "Batch [1900/5778] Loss: 0.9794\n",
      "Batch [1910/5778] Loss: 0.9305\n",
      "Batch [1920/5778] Loss: 0.9292\n",
      "Batch [1930/5778] Loss: 0.8859\n",
      "Batch [1940/5778] Loss: 0.9427\n",
      "Batch [1950/5778] Loss: 0.9491\n",
      "Batch [1960/5778] Loss: 0.9505\n",
      "Batch [1970/5778] Loss: 0.9491\n",
      "Batch [1980/5778] Loss: 0.9418\n",
      "Batch [1990/5778] Loss: 0.9543\n",
      "Batch [2000/5778] Loss: 0.9956\n",
      "Batch [2010/5778] Loss: 0.9135\n",
      "Batch [2020/5778] Loss: 0.9682\n",
      "Batch [2030/5778] Loss: 0.9698\n",
      "Batch [2040/5778] Loss: 0.9532\n",
      "Batch [2050/5778] Loss: 0.9403\n",
      "Batch [2060/5778] Loss: 0.9346\n",
      "Batch [2070/5778] Loss: 0.9484\n",
      "Batch [2080/5778] Loss: 0.9468\n",
      "Batch [2090/5778] Loss: 0.9377\n",
      "Batch [2100/5778] Loss: 0.8895\n",
      "Batch [2110/5778] Loss: 0.9553\n",
      "Batch [2120/5778] Loss: 0.9886\n",
      "Batch [2130/5778] Loss: 0.9642\n",
      "Batch [2140/5778] Loss: 0.9688\n",
      "Batch [2150/5778] Loss: 0.8841\n",
      "Batch [2160/5778] Loss: 1.0064\n",
      "Batch [2170/5778] Loss: 0.9433\n",
      "Batch [2180/5778] Loss: 0.9591\n",
      "Batch [2190/5778] Loss: 0.9506\n",
      "Batch [2200/5778] Loss: 0.9453\n",
      "Batch [2210/5778] Loss: 0.9398\n",
      "Batch [2220/5778] Loss: 0.7037\n",
      "Batch [2230/5778] Loss: 0.5331\n",
      "Batch [2240/5778] Loss: 0.5021\n",
      "Batch [2250/5778] Loss: 0.4699\n",
      "Batch [2260/5778] Loss: 0.4473\n",
      "Batch [2270/5778] Loss: 0.4050\n",
      "Batch [2280/5778] Loss: 0.3242\n",
      "Batch [2290/5778] Loss: 0.2628\n",
      "Batch [2300/5778] Loss: 0.3302\n",
      "Batch [2310/5778] Loss: 0.2768\n",
      "Batch [2320/5778] Loss: 0.2166\n",
      "Batch [2330/5778] Loss: 0.2255\n",
      "Batch [2340/5778] Loss: 0.2213\n",
      "Batch [2350/5778] Loss: 0.1626\n",
      "Batch [2360/5778] Loss: 0.1257\n",
      "Batch [2370/5778] Loss: 0.1385\n",
      "Batch [2380/5778] Loss: 0.1379\n",
      "Batch [2390/5778] Loss: 0.1131\n",
      "Batch [2400/5778] Loss: 0.0750\n",
      "Batch [2410/5778] Loss: 0.1246\n",
      "Batch [2420/5778] Loss: 0.0987\n",
      "Batch [2430/5778] Loss: 0.0942\n",
      "Batch [2440/5778] Loss: 0.0868\n",
      "Batch [2450/5778] Loss: 0.0878\n",
      "Batch [2460/5778] Loss: 0.0728\n",
      "Batch [2470/5778] Loss: 0.0681\n",
      "Batch [2480/5778] Loss: 0.0956\n",
      "Batch [2490/5778] Loss: 0.0531\n",
      "Batch [2500/5778] Loss: 0.0836\n",
      "Batch [2510/5778] Loss: 0.0706\n",
      "Batch [2520/5778] Loss: 0.0493\n",
      "Batch [2530/5778] Loss: 0.0576\n",
      "Batch [2540/5778] Loss: 0.0504\n",
      "Batch [2550/5778] Loss: 0.0685\n",
      "Batch [2560/5778] Loss: 0.0720\n",
      "Batch [2570/5778] Loss: 0.0830\n",
      "Batch [2580/5778] Loss: 0.0664\n",
      "Batch [2590/5778] Loss: 0.0594\n",
      "Batch [2600/5778] Loss: 0.0619\n",
      "Batch [2610/5778] Loss: 0.0407\n",
      "Batch [2620/5778] Loss: 0.0320\n",
      "Batch [2630/5778] Loss: 0.0532\n",
      "Batch [2640/5778] Loss: 0.0676\n",
      "Batch [2650/5778] Loss: 0.0639\n",
      "Batch [2660/5778] Loss: 0.0527\n",
      "Batch [2670/5778] Loss: 0.0539\n",
      "Batch [2680/5778] Loss: 0.0488\n",
      "Batch [2690/5778] Loss: 0.0427\n",
      "Batch [2700/5778] Loss: 0.0450\n",
      "Batch [2710/5778] Loss: 0.0529\n",
      "Batch [2720/5778] Loss: 0.0432\n",
      "Batch [2730/5778] Loss: 0.0570\n",
      "Batch [2740/5778] Loss: 0.0343\n",
      "Batch [2750/5778] Loss: 0.0415\n",
      "Batch [2760/5778] Loss: 0.0365\n",
      "Batch [2770/5778] Loss: 0.0343\n",
      "Batch [2780/5778] Loss: 0.0538\n",
      "Batch [2790/5778] Loss: 0.0534\n",
      "Batch [2800/5778] Loss: 0.0416\n",
      "Batch [2810/5778] Loss: 0.0343\n",
      "Batch [2820/5778] Loss: 0.0462\n",
      "Batch [2830/5778] Loss: 0.0512\n",
      "Batch [2840/5778] Loss: 0.0450\n",
      "Batch [2850/5778] Loss: 0.0553\n",
      "Batch [2860/5778] Loss: 0.0288\n",
      "Batch [2870/5778] Loss: 0.0463\n",
      "Batch [2880/5778] Loss: 0.0464\n",
      "Batch [2890/5778] Loss: 0.0430\n",
      "Batch [2900/5778] Loss: 0.0424\n",
      "Batch [2910/5778] Loss: 0.0319\n",
      "Batch [2920/5778] Loss: 0.0252\n",
      "Batch [2930/5778] Loss: 0.0356\n",
      "Batch [2940/5778] Loss: 0.0244\n",
      "Batch [2950/5778] Loss: 0.0398\n",
      "Batch [2960/5778] Loss: 0.0601\n",
      "Batch [2970/5778] Loss: 0.0440\n",
      "Batch [2980/5778] Loss: 0.0295\n",
      "Batch [2990/5778] Loss: 0.0270\n",
      "Batch [3000/5778] Loss: 0.0532\n",
      "Batch [3010/5778] Loss: 0.0371\n",
      "Batch [3020/5778] Loss: 0.0252\n",
      "Batch [3030/5778] Loss: 0.0311\n",
      "Batch [3040/5778] Loss: 0.0308\n",
      "Batch [3050/5778] Loss: 0.0358\n",
      "Batch [3060/5778] Loss: 0.0194\n",
      "Batch [3070/5778] Loss: 0.0799\n",
      "Batch [3080/5778] Loss: 0.0410\n",
      "Batch [3090/5778] Loss: 0.0287\n",
      "Batch [3100/5778] Loss: 0.0332\n",
      "Batch [3110/5778] Loss: 0.0270\n",
      "Batch [3120/5778] Loss: 0.0425\n",
      "Batch [3130/5778] Loss: 0.0328\n",
      "Batch [3140/5778] Loss: 0.0282\n",
      "Batch [3150/5778] Loss: 0.0380\n",
      "Batch [3160/5778] Loss: 0.0371\n",
      "Batch [3170/5778] Loss: 0.0454\n",
      "Batch [3180/5778] Loss: 0.0352\n",
      "Batch [3190/5778] Loss: 0.0383\n",
      "Batch [3200/5778] Loss: 0.0359\n",
      "Batch [3210/5778] Loss: 0.0376\n",
      "Batch [3220/5778] Loss: 0.0250\n",
      "Batch [3230/5778] Loss: 0.0276\n",
      "Batch [3240/5778] Loss: 0.0350\n",
      "Batch [3250/5778] Loss: 0.0348\n",
      "Batch [3260/5778] Loss: 0.0208\n",
      "Batch [3270/5778] Loss: 0.0380\n",
      "Batch [3280/5778] Loss: 0.0292\n",
      "Batch [3290/5778] Loss: 0.0219\n",
      "Batch [3300/5778] Loss: 0.0382\n",
      "Batch [3310/5778] Loss: 0.0270\n",
      "Batch [3320/5778] Loss: 0.0201\n",
      "Batch [3330/5778] Loss: 0.0252\n",
      "Batch [3340/5778] Loss: 0.0352\n",
      "Batch [3350/5778] Loss: 0.0425\n",
      "Batch [3360/5778] Loss: 0.0532\n",
      "Batch [3370/5778] Loss: 0.0174\n",
      "Batch [3380/5778] Loss: 0.0290\n",
      "Batch [3390/5778] Loss: 0.0334\n",
      "Batch [3400/5778] Loss: 0.0305\n",
      "Batch [3410/5778] Loss: 0.0246\n",
      "Batch [3420/5778] Loss: 0.0311\n",
      "Batch [3430/5778] Loss: 0.0314\n",
      "Batch [3440/5778] Loss: 0.0345\n",
      "Batch [3450/5778] Loss: 0.0200\n",
      "Batch [3460/5778] Loss: 0.0217\n",
      "Batch [3470/5778] Loss: 0.0394\n",
      "Batch [3480/5778] Loss: 0.0243\n",
      "Batch [3490/5778] Loss: 0.0288\n",
      "Batch [3500/5778] Loss: 0.0285\n",
      "Batch [3510/5778] Loss: 0.0308\n",
      "Batch [3520/5778] Loss: 0.0194\n",
      "Batch [3530/5778] Loss: 0.0391\n",
      "Batch [3540/5778] Loss: 0.0294\n",
      "Batch [3550/5778] Loss: 0.0336\n",
      "Batch [3560/5778] Loss: 0.0349\n",
      "Batch [3570/5778] Loss: 0.0391\n",
      "Batch [3580/5778] Loss: 0.0172\n",
      "Batch [3590/5778] Loss: 0.0267\n",
      "Batch [3600/5778] Loss: 0.0245\n",
      "Batch [3610/5778] Loss: 0.0244\n",
      "Batch [3620/5778] Loss: 0.0329\n",
      "Batch [3630/5778] Loss: 0.0161\n",
      "Batch [3640/5778] Loss: 0.0405\n",
      "Batch [3650/5778] Loss: 0.0362\n",
      "Batch [3660/5778] Loss: 0.0273\n",
      "Batch [3670/5778] Loss: 0.0214\n",
      "Batch [3680/5778] Loss: 0.0275\n",
      "Batch [3690/5778] Loss: 0.0218\n",
      "Batch [3700/5778] Loss: 0.0336\n",
      "Batch [3710/5778] Loss: 0.0222\n",
      "Batch [3720/5778] Loss: 0.0252\n",
      "Batch [3730/5778] Loss: 0.0216\n",
      "Batch [3740/5778] Loss: 0.0185\n",
      "Batch [3750/5778] Loss: 0.0245\n",
      "Batch [3760/5778] Loss: 0.0170\n",
      "Batch [3770/5778] Loss: 0.0361\n",
      "Batch [3780/5778] Loss: 0.0175\n",
      "Batch [3790/5778] Loss: 0.0202\n",
      "Batch [3800/5778] Loss: 0.0246\n",
      "Batch [3810/5778] Loss: 0.0213\n",
      "Batch [3820/5778] Loss: 0.0324\n",
      "Batch [3830/5778] Loss: 0.0227\n",
      "Batch [3840/5778] Loss: 0.0194\n",
      "Batch [3850/5778] Loss: 0.0356\n",
      "Batch [3860/5778] Loss: 0.0286\n",
      "Batch [3870/5778] Loss: 0.0212\n",
      "Batch [3880/5778] Loss: 0.0242\n",
      "Batch [3890/5778] Loss: 0.0213\n",
      "Batch [3900/5778] Loss: 0.0286\n",
      "Batch [3910/5778] Loss: 0.0306\n",
      "Batch [3920/5778] Loss: 0.0214\n",
      "Batch [3930/5778] Loss: 0.0248\n",
      "Batch [3940/5778] Loss: 0.0143\n",
      "Batch [3950/5778] Loss: 0.0173\n",
      "Batch [3960/5778] Loss: 0.0215\n",
      "Batch [3970/5778] Loss: 0.0378\n",
      "Batch [3980/5778] Loss: 0.0168\n",
      "Batch [3990/5778] Loss: 0.0277\n",
      "Batch [4000/5778] Loss: 0.0233\n",
      "Batch [4010/5778] Loss: 0.0230\n",
      "Batch [4020/5778] Loss: 0.0181\n",
      "Batch [4030/5778] Loss: 0.0283\n",
      "Batch [4040/5778] Loss: 0.0193\n",
      "Batch [4050/5778] Loss: 0.0211\n",
      "Batch [4060/5778] Loss: 0.0171\n",
      "Batch [4070/5778] Loss: 0.0289\n",
      "Batch [4080/5778] Loss: 0.0248\n",
      "Batch [4090/5778] Loss: 0.0251\n",
      "Batch [4100/5778] Loss: 0.0222\n",
      "Batch [4110/5778] Loss: 0.0257\n",
      "Batch [4120/5778] Loss: 0.0293\n",
      "Batch [4130/5778] Loss: 0.0190\n",
      "Batch [4140/5778] Loss: 0.0142\n",
      "Batch [4150/5778] Loss: 0.0237\n",
      "Batch [4160/5778] Loss: 0.0211\n",
      "Batch [4170/5778] Loss: 0.0268\n",
      "Batch [4180/5778] Loss: 0.0221\n",
      "Batch [4190/5778] Loss: 0.0269\n",
      "Batch [4200/5778] Loss: 0.0250\n",
      "Batch [4210/5778] Loss: 0.0318\n",
      "Batch [4220/5778] Loss: 0.0201\n",
      "Batch [4230/5778] Loss: 0.0167\n",
      "Batch [4240/5778] Loss: 0.0208\n",
      "Batch [4250/5778] Loss: 0.0220\n",
      "Batch [4260/5778] Loss: 0.0161\n",
      "Batch [4270/5778] Loss: 0.0131\n",
      "Batch [4280/5778] Loss: 0.0290\n",
      "Batch [4290/5778] Loss: 0.0177\n",
      "Batch [4300/5778] Loss: 0.0366\n",
      "Batch [4310/5778] Loss: 0.0142\n",
      "Batch [4320/5778] Loss: 0.0279\n",
      "Batch [4330/5778] Loss: 0.0281\n",
      "Batch [4340/5778] Loss: 0.0217\n",
      "Batch [4350/5778] Loss: 0.0251\n",
      "Batch [4360/5778] Loss: 0.0314\n",
      "Batch [4370/5778] Loss: 0.0168\n",
      "Batch [4380/5778] Loss: 0.0237\n",
      "Batch [4390/5778] Loss: 0.0196\n",
      "Batch [4400/5778] Loss: 0.0165\n",
      "Batch [4410/5778] Loss: 0.0275\n",
      "Batch [4420/5778] Loss: 0.0259\n",
      "Batch [4430/5778] Loss: 0.0180\n",
      "Batch [4440/5778] Loss: 0.0241\n",
      "Batch [4450/5778] Loss: 0.0244\n",
      "Batch [4460/5778] Loss: 0.0169\n",
      "Batch [4470/5778] Loss: 0.0189\n",
      "Batch [4480/5778] Loss: 0.0150\n",
      "Batch [4490/5778] Loss: 0.0282\n",
      "Batch [4500/5778] Loss: 0.0213\n",
      "Batch [4510/5778] Loss: 0.0140\n",
      "Batch [4520/5778] Loss: 0.0158\n",
      "Batch [4530/5778] Loss: 0.0288\n",
      "Batch [4540/5778] Loss: 0.0147\n",
      "Batch [4550/5778] Loss: 0.0203\n",
      "Batch [4560/5778] Loss: 0.0147\n",
      "Batch [4570/5778] Loss: 0.0306\n",
      "Batch [4580/5778] Loss: 0.0153\n",
      "Batch [4590/5778] Loss: 0.0225\n",
      "Batch [4600/5778] Loss: 0.0154\n",
      "Batch [4610/5778] Loss: 0.0270\n",
      "Batch [4620/5778] Loss: 0.0278\n",
      "Batch [4630/5778] Loss: 0.0208\n",
      "Batch [4640/5778] Loss: 0.0245\n",
      "Batch [4650/5778] Loss: 0.0280\n",
      "Batch [4660/5778] Loss: 0.0324\n",
      "Batch [4670/5778] Loss: 0.0299\n",
      "Batch [4680/5778] Loss: 0.0136\n",
      "Batch [4690/5778] Loss: 0.0280\n",
      "Batch [4700/5778] Loss: 0.0099\n",
      "Batch [4710/5778] Loss: 0.0120\n",
      "Batch [4720/5778] Loss: 0.0295\n",
      "Batch [4730/5778] Loss: 0.0091\n",
      "Batch [4740/5778] Loss: 0.0139\n",
      "Batch [4750/5778] Loss: 0.0263\n",
      "Batch [4760/5778] Loss: 0.0153\n",
      "Batch [4770/5778] Loss: 0.0192\n",
      "Batch [4780/5778] Loss: 0.0172\n",
      "Batch [4790/5778] Loss: 0.0180\n",
      "Batch [4800/5778] Loss: 0.0261\n",
      "Batch [4810/5778] Loss: 0.0083\n",
      "Batch [4820/5778] Loss: 0.0211\n",
      "Batch [4830/5778] Loss: 0.0179\n",
      "Batch [4840/5778] Loss: 0.0141\n",
      "Batch [4850/5778] Loss: 0.0134\n",
      "Batch [4860/5778] Loss: 0.0170\n",
      "Batch [4870/5778] Loss: 0.0256\n",
      "Batch [4880/5778] Loss: 0.0344\n",
      "Batch [4890/5778] Loss: 0.0217\n",
      "Batch [4900/5778] Loss: 0.0129\n",
      "Batch [4910/5778] Loss: 0.0133\n",
      "Batch [4920/5778] Loss: 0.0185\n",
      "Batch [4930/5778] Loss: 0.0244\n",
      "Batch [4940/5778] Loss: 0.0114\n",
      "Batch [4950/5778] Loss: 0.0087\n",
      "Batch [4960/5778] Loss: 0.0280\n",
      "Batch [4970/5778] Loss: 0.0211\n",
      "Batch [4980/5778] Loss: 0.0281\n",
      "Batch [4990/5778] Loss: 0.0209\n",
      "Batch [5000/5778] Loss: 0.0246\n",
      "Batch [5010/5778] Loss: 0.0205\n",
      "Batch [5020/5778] Loss: 0.0215\n",
      "Batch [5030/5778] Loss: 0.0151\n",
      "Batch [5040/5778] Loss: 0.0233\n",
      "Batch [5050/5778] Loss: 0.0173\n",
      "Batch [5060/5778] Loss: 0.0186\n",
      "Batch [5070/5778] Loss: 0.0199\n",
      "Batch [5080/5778] Loss: 0.0206\n",
      "Batch [5090/5778] Loss: 0.0178\n",
      "Batch [5100/5778] Loss: 0.0132\n",
      "Batch [5110/5778] Loss: 0.0376\n",
      "Batch [5120/5778] Loss: 0.0144\n",
      "Batch [5130/5778] Loss: 0.0189\n",
      "Batch [5140/5778] Loss: 0.0152\n",
      "Batch [5150/5778] Loss: 0.0178\n",
      "Batch [5160/5778] Loss: 0.0170\n",
      "Batch [5170/5778] Loss: 0.0092\n",
      "Batch [5180/5778] Loss: 0.0291\n",
      "Batch [5190/5778] Loss: 0.0126\n",
      "Batch [5200/5778] Loss: 0.0154\n",
      "Batch [5210/5778] Loss: 0.0257\n",
      "Batch [5220/5778] Loss: 0.0214\n",
      "Batch [5230/5778] Loss: 0.0209\n",
      "Batch [5240/5778] Loss: 0.0107\n",
      "Batch [5250/5778] Loss: 0.0140\n",
      "Batch [5260/5778] Loss: 0.0106\n",
      "Batch [5270/5778] Loss: 0.0145\n",
      "Batch [5280/5778] Loss: 0.0083\n",
      "Batch [5290/5778] Loss: 0.0206\n",
      "Batch [5300/5778] Loss: 0.0175\n",
      "Batch [5310/5778] Loss: 0.0127\n",
      "Batch [5320/5778] Loss: 0.0146\n",
      "Batch [5330/5778] Loss: 0.0233\n",
      "Batch [5340/5778] Loss: 0.0137\n",
      "Batch [5350/5778] Loss: 0.0293\n",
      "Batch [5360/5778] Loss: 0.0160\n",
      "Batch [5370/5778] Loss: 0.0235\n",
      "Batch [5380/5778] Loss: 0.0201\n",
      "Batch [5390/5778] Loss: 0.0212\n",
      "Batch [5400/5778] Loss: 0.0140\n",
      "Batch [5410/5778] Loss: 0.0218\n",
      "Batch [5420/5778] Loss: 0.0267\n",
      "Batch [5430/5778] Loss: 0.0244\n",
      "Batch [5440/5778] Loss: 0.0092\n",
      "Batch [5450/5778] Loss: 0.0127\n",
      "Batch [5460/5778] Loss: 0.0128\n",
      "Batch [5470/5778] Loss: 0.0165\n",
      "Batch [5480/5778] Loss: 0.0164\n",
      "Batch [5490/5778] Loss: 0.0197\n",
      "Batch [5500/5778] Loss: 0.0175\n",
      "Batch [5510/5778] Loss: 0.0095\n",
      "Batch [5520/5778] Loss: 0.0227\n",
      "Batch [5530/5778] Loss: 0.0218\n",
      "Batch [5540/5778] Loss: 0.0290\n",
      "Batch [5550/5778] Loss: 0.0119\n",
      "Batch [5560/5778] Loss: 0.0208\n",
      "Batch [5570/5778] Loss: 0.0243\n",
      "Batch [5580/5778] Loss: 0.0150\n",
      "Batch [5590/5778] Loss: 0.0181\n",
      "Batch [5600/5778] Loss: 0.0053\n",
      "Batch [5610/5778] Loss: 0.0220\n",
      "Batch [5620/5778] Loss: 0.0274\n",
      "Batch [5630/5778] Loss: 0.0184\n",
      "Batch [5640/5778] Loss: 0.0073\n",
      "Batch [5650/5778] Loss: 0.0125\n",
      "Batch [5660/5778] Loss: 0.0179\n",
      "Batch [5670/5778] Loss: 0.0196\n",
      "Batch [5680/5778] Loss: 0.0083\n",
      "Batch [5690/5778] Loss: 0.0188\n",
      "Batch [5700/5778] Loss: 0.0234\n",
      "Batch [5710/5778] Loss: 0.0143\n",
      "Batch [5720/5778] Loss: 0.0154\n",
      "Batch [5730/5778] Loss: 0.0191\n",
      "Batch [5740/5778] Loss: 0.0233\n",
      "Batch [5750/5778] Loss: 0.0169\n",
      "Batch [5760/5778] Loss: 0.0313\n",
      "Batch [5770/5778] Loss: 0.0122\n",
      "Epoch [1/5] Average Loss: 0.3893\n",
      "Epoch [1/5] Validation Loss: 0.0164 Accuracy: 99.39%\n",
      "Epoch [2/5]\n",
      "Batch [0/5778] Loss: 0.0160\n",
      "Batch [10/5778] Loss: 0.0223\n",
      "Batch [20/5778] Loss: 0.0133\n",
      "Batch [30/5778] Loss: 0.0121\n",
      "Batch [40/5778] Loss: 0.0083\n",
      "Batch [50/5778] Loss: 0.0168\n",
      "Batch [60/5778] Loss: 0.0139\n",
      "Batch [70/5778] Loss: 0.0139\n",
      "Batch [80/5778] Loss: 0.0156\n",
      "Batch [90/5778] Loss: 0.0256\n",
      "Batch [100/5778] Loss: 0.0310\n",
      "Batch [110/5778] Loss: 0.0157\n",
      "Batch [120/5778] Loss: 0.0094\n",
      "Batch [130/5778] Loss: 0.0190\n",
      "Batch [140/5778] Loss: 0.0172\n",
      "Batch [150/5778] Loss: 0.0148\n",
      "Batch [160/5778] Loss: 0.0176\n",
      "Batch [170/5778] Loss: 0.0205\n",
      "Batch [180/5778] Loss: 0.0288\n",
      "Batch [190/5778] Loss: 0.0265\n",
      "Batch [200/5778] Loss: 0.0099\n",
      "Batch [210/5778] Loss: 0.0256\n",
      "Batch [220/5778] Loss: 0.0137\n",
      "Batch [230/5778] Loss: 0.0222\n",
      "Batch [240/5778] Loss: 0.0193\n",
      "Batch [250/5778] Loss: 0.0205\n",
      "Batch [260/5778] Loss: 0.0183\n",
      "Batch [270/5778] Loss: 0.0273\n",
      "Batch [280/5778] Loss: 0.0213\n",
      "Batch [290/5778] Loss: 0.0270\n",
      "Batch [300/5778] Loss: 0.0107\n",
      "Batch [310/5778] Loss: 0.0102\n",
      "Batch [320/5778] Loss: 0.0123\n",
      "Batch [330/5778] Loss: 0.0276\n",
      "Batch [340/5778] Loss: 0.0094\n",
      "Batch [350/5778] Loss: 0.0301\n",
      "Batch [360/5778] Loss: 0.0098\n",
      "Batch [370/5778] Loss: 0.0163\n",
      "Batch [380/5778] Loss: 0.0140\n",
      "Batch [390/5778] Loss: 0.0154\n",
      "Batch [400/5778] Loss: 0.0172\n",
      "Batch [410/5778] Loss: 0.0260\n",
      "Batch [420/5778] Loss: 0.0108\n",
      "Batch [430/5778] Loss: 0.0095\n",
      "Batch [440/5778] Loss: 0.0174\n",
      "Batch [450/5778] Loss: 0.0205\n",
      "Batch [460/5778] Loss: 0.0204\n",
      "Batch [470/5778] Loss: 0.0185\n",
      "Batch [480/5778] Loss: 0.0207\n",
      "Batch [490/5778] Loss: 0.0139\n",
      "Batch [500/5778] Loss: 0.0143\n",
      "Batch [510/5778] Loss: 0.0184\n",
      "Batch [520/5778] Loss: 0.0122\n",
      "Batch [530/5778] Loss: 0.0207\n",
      "Batch [540/5778] Loss: 0.0266\n",
      "Batch [550/5778] Loss: 0.0241\n",
      "Batch [560/5778] Loss: 0.0109\n",
      "Batch [570/5778] Loss: 0.0099\n",
      "Batch [580/5778] Loss: 0.0133\n",
      "Batch [590/5778] Loss: 0.0095\n",
      "Batch [600/5778] Loss: 0.0123\n",
      "Batch [610/5778] Loss: 0.0102\n",
      "Batch [620/5778] Loss: 0.0100\n",
      "Batch [630/5778] Loss: 0.0166\n",
      "Batch [640/5778] Loss: 0.0380\n",
      "Batch [650/5778] Loss: 0.0133\n",
      "Batch [660/5778] Loss: 0.0163\n",
      "Batch [670/5778] Loss: 0.0089\n",
      "Batch [680/5778] Loss: 0.0139\n",
      "Batch [690/5778] Loss: 0.0169\n",
      "Batch [700/5778] Loss: 0.0095\n",
      "Batch [710/5778] Loss: 0.0119\n",
      "Batch [720/5778] Loss: 0.0148\n",
      "Batch [730/5778] Loss: 0.0251\n",
      "Batch [740/5778] Loss: 0.0197\n",
      "Batch [750/5778] Loss: 0.0203\n",
      "Batch [760/5778] Loss: 0.0229\n",
      "Batch [770/5778] Loss: 0.0147\n",
      "Batch [780/5778] Loss: 0.0134\n",
      "Batch [790/5778] Loss: 0.0164\n",
      "Batch [800/5778] Loss: 0.0224\n",
      "Batch [810/5778] Loss: 0.0220\n",
      "Batch [820/5778] Loss: 0.0183\n",
      "Batch [830/5778] Loss: 0.0160\n",
      "Batch [840/5778] Loss: 0.0193\n",
      "Batch [850/5778] Loss: 0.0076\n",
      "Batch [860/5778] Loss: 0.0271\n",
      "Batch [870/5778] Loss: 0.0096\n",
      "Batch [880/5778] Loss: 0.0154\n",
      "Batch [890/5778] Loss: 0.0157\n",
      "Batch [900/5778] Loss: 0.0238\n",
      "Batch [910/5778] Loss: 0.0112\n",
      "Batch [920/5778] Loss: 0.0213\n",
      "Batch [930/5778] Loss: 0.0174\n",
      "Batch [940/5778] Loss: 0.0109\n",
      "Batch [950/5778] Loss: 0.0168\n",
      "Batch [960/5778] Loss: 0.0116\n",
      "Batch [970/5778] Loss: 0.0142\n",
      "Batch [980/5778] Loss: 0.0121\n",
      "Batch [990/5778] Loss: 0.0172\n",
      "Batch [1000/5778] Loss: 0.0169\n",
      "Batch [1010/5778] Loss: 0.0146\n",
      "Batch [1020/5778] Loss: 0.0198\n",
      "Batch [1030/5778] Loss: 0.0109\n",
      "Batch [1040/5778] Loss: 0.0187\n",
      "Batch [1050/5778] Loss: 0.0308\n",
      "Batch [1060/5778] Loss: 0.0252\n",
      "Batch [1070/5778] Loss: 0.0138\n",
      "Batch [1080/5778] Loss: 0.0174\n",
      "Batch [1090/5778] Loss: 0.0156\n",
      "Batch [1100/5778] Loss: 0.0238\n",
      "Batch [1110/5778] Loss: 0.0152\n",
      "Batch [1120/5778] Loss: 0.0144\n",
      "Batch [1130/5778] Loss: 0.0209\n",
      "Batch [1140/5778] Loss: 0.0214\n",
      "Batch [1150/5778] Loss: 0.0128\n",
      "Batch [1160/5778] Loss: 0.0236\n",
      "Batch [1170/5778] Loss: 0.0148\n",
      "Batch [1180/5778] Loss: 0.0099\n",
      "Batch [1190/5778] Loss: 0.0282\n",
      "Batch [1200/5778] Loss: 0.0065\n",
      "Batch [1210/5778] Loss: 0.0249\n",
      "Batch [1220/5778] Loss: 0.0152\n",
      "Batch [1230/5778] Loss: 0.0122\n",
      "Batch [1240/5778] Loss: 0.0136\n",
      "Batch [1250/5778] Loss: 0.0123\n",
      "Batch [1260/5778] Loss: 0.0160\n",
      "Batch [1270/5778] Loss: 0.0097\n",
      "Batch [1280/5778] Loss: 0.0182\n",
      "Batch [1290/5778] Loss: 0.0092\n",
      "Batch [1300/5778] Loss: 0.0216\n",
      "Batch [1310/5778] Loss: 0.0069\n",
      "Batch [1320/5778] Loss: 0.0062\n",
      "Batch [1330/5778] Loss: 0.0112\n",
      "Batch [1340/5778] Loss: 0.0093\n",
      "Batch [1350/5778] Loss: 0.0170\n",
      "Batch [1360/5778] Loss: 0.0186\n",
      "Batch [1370/5778] Loss: 0.0124\n",
      "Batch [1380/5778] Loss: 0.0168\n",
      "Batch [1390/5778] Loss: 0.0200\n",
      "Batch [1400/5778] Loss: 0.0138\n",
      "Batch [1410/5778] Loss: 0.0133\n",
      "Batch [1420/5778] Loss: 0.0183\n",
      "Batch [1430/5778] Loss: 0.0082\n",
      "Batch [1440/5778] Loss: 0.0149\n",
      "Batch [1450/5778] Loss: 0.0147\n",
      "Batch [1460/5778] Loss: 0.0118\n",
      "Batch [1470/5778] Loss: 0.0178\n",
      "Batch [1480/5778] Loss: 0.0173\n",
      "Batch [1490/5778] Loss: 0.0083\n",
      "Batch [1500/5778] Loss: 0.0092\n",
      "Batch [1510/5778] Loss: 0.0258\n",
      "Batch [1520/5778] Loss: 0.0170\n",
      "Batch [1530/5778] Loss: 0.0165\n",
      "Batch [1540/5778] Loss: 0.0076\n",
      "Batch [1550/5778] Loss: 0.0131\n",
      "Batch [1560/5778] Loss: 0.0102\n",
      "Batch [1570/5778] Loss: 0.0144\n",
      "Batch [1580/5778] Loss: 0.0162\n",
      "Batch [1590/5778] Loss: 0.0154\n",
      "Batch [1600/5778] Loss: 0.0052\n",
      "Batch [1610/5778] Loss: 0.0210\n",
      "Batch [1620/5778] Loss: 0.0096\n",
      "Batch [1630/5778] Loss: 0.0193\n",
      "Batch [1640/5778] Loss: 0.0195\n",
      "Batch [1650/5778] Loss: 0.0099\n",
      "Batch [1660/5778] Loss: 0.0175\n",
      "Batch [1670/5778] Loss: 0.0143\n",
      "Batch [1680/5778] Loss: 0.0092\n",
      "Batch [1690/5778] Loss: 0.0258\n",
      "Batch [1700/5778] Loss: 0.0123\n",
      "Batch [1710/5778] Loss: 0.0149\n",
      "Batch [1720/5778] Loss: 0.0079\n",
      "Batch [1730/5778] Loss: 0.0119\n",
      "Batch [1740/5778] Loss: 0.0181\n",
      "Batch [1750/5778] Loss: 0.0057\n",
      "Batch [1760/5778] Loss: 0.0101\n",
      "Batch [1770/5778] Loss: 0.0081\n",
      "Batch [1780/5778] Loss: 0.0126\n",
      "Batch [1790/5778] Loss: 0.0100\n",
      "Batch [1800/5778] Loss: 0.0080\n",
      "Batch [1810/5778] Loss: 0.0134\n",
      "Batch [1820/5778] Loss: 0.0146\n",
      "Batch [1830/5778] Loss: 0.0170\n",
      "Batch [1840/5778] Loss: 0.0066\n",
      "Batch [1850/5778] Loss: 0.0100\n",
      "Batch [1860/5778] Loss: 0.0190\n",
      "Batch [1870/5778] Loss: 0.0149\n",
      "Batch [1880/5778] Loss: 0.0168\n",
      "Batch [1890/5778] Loss: 0.0107\n",
      "Batch [1900/5778] Loss: 0.0195\n",
      "Batch [1910/5778] Loss: 0.0105\n",
      "Batch [1920/5778] Loss: 0.0071\n",
      "Batch [1930/5778] Loss: 0.0208\n",
      "Batch [1940/5778] Loss: 0.0140\n",
      "Batch [1950/5778] Loss: 0.0051\n",
      "Batch [1960/5778] Loss: 0.0097\n",
      "Batch [1970/5778] Loss: 0.0155\n",
      "Batch [1980/5778] Loss: 0.0084\n",
      "Batch [1990/5778] Loss: 0.0191\n",
      "Batch [2000/5778] Loss: 0.0052\n",
      "Batch [2010/5778] Loss: 0.0096\n",
      "Batch [2020/5778] Loss: 0.0135\n",
      "Batch [2030/5778] Loss: 0.0184\n",
      "Batch [2040/5778] Loss: 0.0235\n",
      "Batch [2050/5778] Loss: 0.0185\n",
      "Batch [2060/5778] Loss: 0.0151\n",
      "Batch [2070/5778] Loss: 0.0253\n",
      "Batch [2080/5778] Loss: 0.0161\n",
      "Batch [2090/5778] Loss: 0.0096\n",
      "Batch [2100/5778] Loss: 0.0150\n",
      "Batch [2110/5778] Loss: 0.0083\n",
      "Batch [2120/5778] Loss: 0.0099\n",
      "Batch [2130/5778] Loss: 0.0095\n",
      "Batch [2140/5778] Loss: 0.0110\n",
      "Batch [2150/5778] Loss: 0.0120\n",
      "Batch [2160/5778] Loss: 0.0140\n",
      "Batch [2170/5778] Loss: 0.0093\n",
      "Batch [2180/5778] Loss: 0.0126\n",
      "Batch [2190/5778] Loss: 0.0164\n",
      "Batch [2200/5778] Loss: 0.0158\n",
      "Batch [2210/5778] Loss: 0.0128\n",
      "Batch [2220/5778] Loss: 0.0174\n",
      "Batch [2230/5778] Loss: 0.0094\n",
      "Batch [2240/5778] Loss: 0.0198\n",
      "Batch [2250/5778] Loss: 0.0090\n",
      "Batch [2260/5778] Loss: 0.0253\n",
      "Batch [2270/5778] Loss: 0.0116\n",
      "Batch [2280/5778] Loss: 0.0130\n",
      "Batch [2290/5778] Loss: 0.0233\n",
      "Batch [2300/5778] Loss: 0.0037\n",
      "Batch [2310/5778] Loss: 0.0154\n",
      "Batch [2320/5778] Loss: 0.0089\n",
      "Batch [2330/5778] Loss: 0.0066\n",
      "Batch [2340/5778] Loss: 0.0114\n",
      "Batch [2350/5778] Loss: 0.0143\n",
      "Batch [2360/5778] Loss: 0.0141\n",
      "Batch [2370/5778] Loss: 0.0094\n",
      "Batch [2380/5778] Loss: 0.0157\n",
      "Batch [2390/5778] Loss: 0.0180\n",
      "Batch [2400/5778] Loss: 0.0087\n",
      "Batch [2410/5778] Loss: 0.0205\n",
      "Batch [2420/5778] Loss: 0.0129\n",
      "Batch [2430/5778] Loss: 0.0227\n",
      "Batch [2440/5778] Loss: 0.0087\n",
      "Batch [2450/5778] Loss: 0.0227\n",
      "Batch [2460/5778] Loss: 0.0186\n",
      "Batch [2470/5778] Loss: 0.0121\n",
      "Batch [2480/5778] Loss: 0.0067\n",
      "Batch [2490/5778] Loss: 0.0194\n",
      "Batch [2500/5778] Loss: 0.0114\n",
      "Batch [2510/5778] Loss: 0.0145\n",
      "Batch [2520/5778] Loss: 0.0150\n",
      "Batch [2530/5778] Loss: 0.0142\n",
      "Batch [2540/5778] Loss: 0.0102\n",
      "Batch [2550/5778] Loss: 0.0103\n",
      "Batch [2560/5778] Loss: 0.0109\n",
      "Batch [2570/5778] Loss: 0.0066\n",
      "Batch [2580/5778] Loss: 0.0202\n",
      "Batch [2590/5778] Loss: 0.0122\n",
      "Batch [2600/5778] Loss: 0.0084\n",
      "Batch [2610/5778] Loss: 0.0127\n",
      "Batch [2620/5778] Loss: 0.0314\n",
      "Batch [2630/5778] Loss: 0.0128\n",
      "Batch [2640/5778] Loss: 0.0133\n",
      "Batch [2650/5778] Loss: 0.0214\n",
      "Batch [2660/5778] Loss: 0.0209\n",
      "Batch [2670/5778] Loss: 0.0155\n",
      "Batch [2680/5778] Loss: 0.0221\n",
      "Batch [2690/5778] Loss: 0.0211\n",
      "Batch [2700/5778] Loss: 0.0142\n",
      "Batch [2710/5778] Loss: 0.0103\n",
      "Batch [2720/5778] Loss: 0.0170\n",
      "Batch [2730/5778] Loss: 0.0102\n",
      "Batch [2740/5778] Loss: 0.0199\n",
      "Batch [2750/5778] Loss: 0.0070\n",
      "Batch [2760/5778] Loss: 0.0121\n",
      "Batch [2770/5778] Loss: 0.0390\n",
      "Batch [2780/5778] Loss: 0.0229\n",
      "Batch [2790/5778] Loss: 0.0264\n",
      "Batch [2800/5778] Loss: 0.0180\n",
      "Batch [2810/5778] Loss: 0.0279\n",
      "Batch [2820/5778] Loss: 0.0153\n",
      "Batch [2830/5778] Loss: 0.0147\n",
      "Batch [2840/5778] Loss: 0.0119\n",
      "Batch [2850/5778] Loss: 0.0122\n",
      "Batch [2860/5778] Loss: 0.0265\n",
      "Batch [2870/5778] Loss: 0.0232\n",
      "Batch [2880/5778] Loss: 0.0309\n",
      "Batch [2890/5778] Loss: 0.0164\n",
      "Batch [2900/5778] Loss: 0.0188\n",
      "Batch [2910/5778] Loss: 0.0222\n",
      "Batch [2920/5778] Loss: 0.0096\n",
      "Batch [2930/5778] Loss: 0.0156\n",
      "Batch [2940/5778] Loss: 0.0257\n",
      "Batch [2950/5778] Loss: 0.0135\n",
      "Batch [2960/5778] Loss: 0.0126\n",
      "Batch [2970/5778] Loss: 0.0147\n",
      "Batch [2980/5778] Loss: 0.0094\n",
      "Batch [2990/5778] Loss: 0.0215\n",
      "Batch [3000/5778] Loss: 0.0085\n",
      "Batch [3010/5778] Loss: 0.0129\n",
      "Batch [3020/5778] Loss: 0.0112\n",
      "Batch [3030/5778] Loss: 0.0277\n",
      "Batch [3040/5778] Loss: 0.0162\n",
      "Batch [3050/5778] Loss: 0.0197\n",
      "Batch [3060/5778] Loss: 0.0139\n",
      "Batch [3070/5778] Loss: 0.0258\n",
      "Batch [3080/5778] Loss: 0.0181\n",
      "Batch [3090/5778] Loss: 0.0254\n",
      "Batch [3100/5778] Loss: 0.0216\n",
      "Batch [3110/5778] Loss: 0.0064\n",
      "Batch [3120/5778] Loss: 0.0154\n",
      "Batch [3130/5778] Loss: 0.0372\n",
      "Batch [3140/5778] Loss: 0.0243\n",
      "Batch [3150/5778] Loss: 0.0107\n",
      "Batch [3160/5778] Loss: 0.0217\n",
      "Batch [3170/5778] Loss: 0.0100\n",
      "Batch [3180/5778] Loss: 0.0111\n",
      "Batch [3190/5778] Loss: 0.0298\n",
      "Batch [3200/5778] Loss: 0.0154\n",
      "Batch [3210/5778] Loss: 0.0065\n",
      "Batch [3220/5778] Loss: 0.0183\n",
      "Batch [3230/5778] Loss: 0.0093\n",
      "Batch [3240/5778] Loss: 0.0112\n",
      "Batch [3250/5778] Loss: 0.0218\n",
      "Batch [3260/5778] Loss: 0.0135\n",
      "Batch [3270/5778] Loss: 0.0121\n",
      "Batch [3280/5778] Loss: 0.0088\n",
      "Batch [3290/5778] Loss: 0.0212\n",
      "Batch [3300/5778] Loss: 0.0127\n",
      "Batch [3310/5778] Loss: 0.0181\n",
      "Batch [3320/5778] Loss: 0.0252\n",
      "Batch [3330/5778] Loss: 0.0181\n",
      "Batch [3340/5778] Loss: 0.0246\n",
      "Batch [3350/5778] Loss: 0.0271\n",
      "Batch [3360/5778] Loss: 0.0163\n",
      "Batch [3370/5778] Loss: 0.0193\n",
      "Batch [3380/5778] Loss: 0.0158\n",
      "Batch [3390/5778] Loss: 0.0178\n",
      "Batch [3400/5778] Loss: 0.0097\n",
      "Batch [3410/5778] Loss: 0.0204\n",
      "Batch [3420/5778] Loss: 0.0179\n",
      "Batch [3430/5778] Loss: 0.0157\n",
      "Batch [3440/5778] Loss: 0.0087\n",
      "Batch [3450/5778] Loss: 0.0148\n",
      "Batch [3460/5778] Loss: 0.0130\n",
      "Batch [3470/5778] Loss: 0.0170\n",
      "Batch [3480/5778] Loss: 0.0127\n",
      "Batch [3490/5778] Loss: 0.0299\n",
      "Batch [3500/5778] Loss: 0.0219\n",
      "Batch [3510/5778] Loss: 0.0063\n",
      "Batch [3520/5778] Loss: 0.0123\n",
      "Batch [3530/5778] Loss: 0.0087\n",
      "Batch [3540/5778] Loss: 0.0133\n",
      "Batch [3550/5778] Loss: 0.0059\n",
      "Batch [3560/5778] Loss: 0.0108\n",
      "Batch [3570/5778] Loss: 0.0088\n",
      "Batch [3580/5778] Loss: 0.0113\n",
      "Batch [3590/5778] Loss: 0.0182\n",
      "Batch [3600/5778] Loss: 0.0174\n",
      "Batch [3610/5778] Loss: 0.0096\n",
      "Batch [3620/5778] Loss: 0.0139\n",
      "Batch [3630/5778] Loss: 0.0085\n",
      "Batch [3640/5778] Loss: 0.0157\n",
      "Batch [3650/5778] Loss: 0.0247\n",
      "Batch [3660/5778] Loss: 0.0170\n",
      "Batch [3670/5778] Loss: 0.0091\n",
      "Batch [3680/5778] Loss: 0.0171\n",
      "Batch [3690/5778] Loss: 0.0164\n",
      "Batch [3700/5778] Loss: 0.0070\n",
      "Batch [3710/5778] Loss: 0.0156\n",
      "Batch [3720/5778] Loss: 0.0173\n",
      "Batch [3730/5778] Loss: 0.0252\n",
      "Batch [3740/5778] Loss: 0.0069\n",
      "Batch [3750/5778] Loss: 0.0243\n",
      "Batch [3760/5778] Loss: 0.0201\n",
      "Batch [3770/5778] Loss: 0.0200\n",
      "Batch [3780/5778] Loss: 0.0192\n",
      "Batch [3790/5778] Loss: 0.0084\n",
      "Batch [3800/5778] Loss: 0.0150\n",
      "Batch [3810/5778] Loss: 0.0078\n",
      "Batch [3820/5778] Loss: 0.0096\n",
      "Batch [3830/5778] Loss: 0.0150\n",
      "Batch [3840/5778] Loss: 0.0150\n",
      "Batch [3850/5778] Loss: 0.0185\n",
      "Batch [3860/5778] Loss: 0.0117\n",
      "Batch [3870/5778] Loss: 0.0101\n",
      "Batch [3880/5778] Loss: 0.0152\n",
      "Batch [3890/5778] Loss: 0.0143\n",
      "Batch [3900/5778] Loss: 0.0195\n",
      "Batch [3910/5778] Loss: 0.0185\n",
      "Batch [3920/5778] Loss: 0.0082\n",
      "Batch [3930/5778] Loss: 0.0133\n",
      "Batch [3940/5778] Loss: 0.0281\n",
      "Batch [3950/5778] Loss: 0.0120\n",
      "Batch [3960/5778] Loss: 0.0175\n",
      "Batch [3970/5778] Loss: 0.0239\n",
      "Batch [3980/5778] Loss: 0.0138\n",
      "Batch [3990/5778] Loss: 0.0116\n",
      "Batch [4000/5778] Loss: 0.0098\n",
      "Batch [4010/5778] Loss: 0.0155\n",
      "Batch [4020/5778] Loss: 0.0122\n",
      "Batch [4030/5778] Loss: 0.0242\n",
      "Batch [4040/5778] Loss: 0.0084\n",
      "Batch [4050/5778] Loss: 0.0091\n",
      "Batch [4060/5778] Loss: 0.0141\n",
      "Batch [4070/5778] Loss: 0.0194\n",
      "Batch [4080/5778] Loss: 0.0288\n",
      "Batch [4090/5778] Loss: 0.0080\n",
      "Batch [4100/5778] Loss: 0.0210\n",
      "Batch [4110/5778] Loss: 0.0200\n",
      "Batch [4120/5778] Loss: 0.0070\n",
      "Batch [4130/5778] Loss: 0.0123\n",
      "Batch [4140/5778] Loss: 0.0085\n",
      "Batch [4150/5778] Loss: 0.0249\n",
      "Batch [4160/5778] Loss: 0.0121\n",
      "Batch [4170/5778] Loss: 0.0128\n",
      "Batch [4180/5778] Loss: 0.0221\n",
      "Batch [4190/5778] Loss: 0.0189\n",
      "Batch [4200/5778] Loss: 0.0098\n",
      "Batch [4210/5778] Loss: 0.0210\n",
      "Batch [4220/5778] Loss: 0.0152\n",
      "Batch [4230/5778] Loss: 0.0129\n",
      "Batch [4240/5778] Loss: 0.0071\n",
      "Batch [4250/5778] Loss: 0.0153\n",
      "Batch [4260/5778] Loss: 0.0111\n",
      "Batch [4270/5778] Loss: 0.0117\n",
      "Batch [4280/5778] Loss: 0.0122\n",
      "Batch [4290/5778] Loss: 0.0094\n",
      "Batch [4300/5778] Loss: 0.0172\n",
      "Batch [4310/5778] Loss: 0.0170\n",
      "Batch [4320/5778] Loss: 0.0239\n",
      "Batch [4330/5778] Loss: 0.0164\n",
      "Batch [4340/5778] Loss: 0.0142\n",
      "Batch [4350/5778] Loss: 0.0153\n",
      "Batch [4360/5778] Loss: 0.0191\n",
      "Batch [4370/5778] Loss: 0.0161\n",
      "Batch [4380/5778] Loss: 0.0068\n",
      "Batch [4390/5778] Loss: 0.0170\n",
      "Batch [4400/5778] Loss: 0.0079\n",
      "Batch [4410/5778] Loss: 0.0097\n",
      "Batch [4420/5778] Loss: 0.0174\n",
      "Batch [4430/5778] Loss: 0.0044\n",
      "Batch [4440/5778] Loss: 0.0201\n",
      "Batch [4450/5778] Loss: 0.0080\n",
      "Batch [4460/5778] Loss: 0.0093\n",
      "Batch [4470/5778] Loss: 0.0138\n",
      "Batch [4480/5778] Loss: 0.0224\n",
      "Batch [4490/5778] Loss: 0.0190\n",
      "Batch [4500/5778] Loss: 0.0164\n",
      "Batch [4510/5778] Loss: 0.0181\n",
      "Batch [4520/5778] Loss: 0.0112\n",
      "Batch [4530/5778] Loss: 0.0174\n",
      "Batch [4540/5778] Loss: 0.0201\n",
      "Batch [4550/5778] Loss: 0.0159\n",
      "Batch [4560/5778] Loss: 0.0079\n",
      "Batch [4570/5778] Loss: 0.0119\n",
      "Batch [4580/5778] Loss: 0.0178\n",
      "Batch [4590/5778] Loss: 0.0105\n",
      "Batch [4600/5778] Loss: 0.0092\n",
      "Batch [4610/5778] Loss: 0.0076\n",
      "Batch [4620/5778] Loss: 0.0119\n",
      "Batch [4630/5778] Loss: 0.0165\n",
      "Batch [4640/5778] Loss: 0.0172\n",
      "Batch [4650/5778] Loss: 0.0102\n",
      "Batch [4660/5778] Loss: 0.0143\n",
      "Batch [4670/5778] Loss: 0.0196\n",
      "Batch [4680/5778] Loss: 0.0088\n",
      "Batch [4690/5778] Loss: 0.0139\n",
      "Batch [4700/5778] Loss: 0.0115\n",
      "Batch [4710/5778] Loss: 0.0167\n",
      "Batch [4720/5778] Loss: 0.0208\n",
      "Batch [4730/5778] Loss: 0.0118\n",
      "Batch [4740/5778] Loss: 0.0097\n",
      "Batch [4750/5778] Loss: 0.0192\n",
      "Batch [4760/5778] Loss: 0.0210\n",
      "Batch [4770/5778] Loss: 0.0115\n",
      "Batch [4780/5778] Loss: 0.0116\n",
      "Batch [4790/5778] Loss: 0.0201\n",
      "Batch [4800/5778] Loss: 0.0127\n",
      "Batch [4810/5778] Loss: 0.0056\n",
      "Batch [4820/5778] Loss: 0.0108\n",
      "Batch [4830/5778] Loss: 0.0132\n",
      "Batch [4840/5778] Loss: 0.0070\n",
      "Batch [4850/5778] Loss: 0.0188\n",
      "Batch [4860/5778] Loss: 0.0178\n",
      "Batch [4870/5778] Loss: 0.0146\n",
      "Batch [4880/5778] Loss: 0.0133\n",
      "Batch [4890/5778] Loss: 0.0122\n",
      "Batch [4900/5778] Loss: 0.0066\n",
      "Batch [4910/5778] Loss: 0.0127\n",
      "Batch [4920/5778] Loss: 0.0088\n",
      "Batch [4930/5778] Loss: 0.0070\n",
      "Batch [4940/5778] Loss: 0.0182\n",
      "Batch [4950/5778] Loss: 0.0145\n",
      "Batch [4960/5778] Loss: 0.0163\n",
      "Batch [4970/5778] Loss: 0.0096\n",
      "Batch [4980/5778] Loss: 0.0144\n",
      "Batch [4990/5778] Loss: 0.0082\n",
      "Batch [5000/5778] Loss: 0.0082\n",
      "Batch [5010/5778] Loss: 0.0123\n",
      "Batch [5020/5778] Loss: 0.0081\n",
      "Batch [5030/5778] Loss: 0.0098\n",
      "Batch [5040/5778] Loss: 0.0080\n",
      "Batch [5050/5778] Loss: 0.0125\n",
      "Batch [5060/5778] Loss: 0.0074\n",
      "Batch [5070/5778] Loss: 0.0137\n",
      "Batch [5080/5778] Loss: 0.0102\n",
      "Batch [5090/5778] Loss: 0.0159\n",
      "Batch [5100/5778] Loss: 0.0100\n",
      "Batch [5110/5778] Loss: 0.0142\n",
      "Batch [5120/5778] Loss: 0.0206\n",
      "Batch [5130/5778] Loss: 0.0218\n",
      "Batch [5140/5778] Loss: 0.0107\n",
      "Batch [5150/5778] Loss: 0.0184\n",
      "Batch [5160/5778] Loss: 0.0065\n",
      "Batch [5170/5778] Loss: 0.0096\n",
      "Batch [5180/5778] Loss: 0.0066\n",
      "Batch [5190/5778] Loss: 0.0194\n",
      "Batch [5200/5778] Loss: 0.0108\n",
      "Batch [5210/5778] Loss: 0.0074\n",
      "Batch [5220/5778] Loss: 0.0117\n",
      "Batch [5230/5778] Loss: 0.0182\n",
      "Batch [5240/5778] Loss: 0.0121\n",
      "Batch [5250/5778] Loss: 0.0253\n",
      "Batch [5260/5778] Loss: 0.0222\n",
      "Batch [5270/5778] Loss: 0.0098\n",
      "Batch [5280/5778] Loss: 0.0073\n",
      "Batch [5290/5778] Loss: 0.0106\n",
      "Batch [5300/5778] Loss: 0.0071\n",
      "Batch [5310/5778] Loss: 0.0136\n",
      "Batch [5320/5778] Loss: 0.0174\n",
      "Batch [5330/5778] Loss: 0.0057\n",
      "Batch [5340/5778] Loss: 0.0120\n",
      "Batch [5350/5778] Loss: 0.0070\n",
      "Batch [5360/5778] Loss: 0.0194\n",
      "Batch [5370/5778] Loss: 0.0124\n",
      "Batch [5380/5778] Loss: 0.0174\n",
      "Batch [5390/5778] Loss: 0.0041\n",
      "Batch [5400/5778] Loss: 0.0083\n",
      "Batch [5410/5778] Loss: 0.0170\n",
      "Batch [5420/5778] Loss: 0.0151\n",
      "Batch [5430/5778] Loss: 0.0214\n",
      "Batch [5440/5778] Loss: 0.0178\n",
      "Batch [5450/5778] Loss: 0.0142\n",
      "Batch [5460/5778] Loss: 0.0153\n",
      "Batch [5470/5778] Loss: 0.0097\n",
      "Batch [5480/5778] Loss: 0.0192\n",
      "Batch [5490/5778] Loss: 0.0128\n",
      "Batch [5500/5778] Loss: 0.0127\n",
      "Batch [5510/5778] Loss: 0.0171\n",
      "Batch [5520/5778] Loss: 0.0112\n",
      "Batch [5530/5778] Loss: 0.0082\n",
      "Batch [5540/5778] Loss: 0.0091\n",
      "Batch [5550/5778] Loss: 0.0098\n",
      "Batch [5560/5778] Loss: 0.0212\n",
      "Batch [5570/5778] Loss: 0.0134\n",
      "Batch [5580/5778] Loss: 0.0074\n",
      "Batch [5590/5778] Loss: 0.0222\n",
      "Batch [5600/5778] Loss: 0.0098\n",
      "Batch [5610/5778] Loss: 0.0134\n",
      "Batch [5620/5778] Loss: 0.0065\n",
      "Batch [5630/5778] Loss: 0.0128\n",
      "Batch [5640/5778] Loss: 0.0140\n",
      "Batch [5650/5778] Loss: 0.0064\n",
      "Batch [5660/5778] Loss: 0.0133\n",
      "Batch [5670/5778] Loss: 0.0053\n",
      "Batch [5680/5778] Loss: 0.0066\n",
      "Batch [5690/5778] Loss: 0.0077\n",
      "Batch [5700/5778] Loss: 0.0088\n",
      "Batch [5710/5778] Loss: 0.0101\n",
      "Batch [5720/5778] Loss: 0.0106\n",
      "Batch [5730/5778] Loss: 0.0105\n",
      "Batch [5740/5778] Loss: 0.0064\n",
      "Batch [5750/5778] Loss: 0.0162\n",
      "Batch [5760/5778] Loss: 0.0078\n",
      "Batch [5770/5778] Loss: 0.0138\n",
      "Epoch [2/5] Average Loss: 0.0148\n",
      "Epoch [2/5] Validation Loss: 0.0121 Accuracy: 99.54%\n",
      "Epoch [3/5]\n",
      "Batch [0/5778] Loss: 0.0101\n",
      "Batch [10/5778] Loss: 0.0111\n",
      "Batch [20/5778] Loss: 0.0124\n",
      "Batch [30/5778] Loss: 0.0193\n",
      "Batch [40/5778] Loss: 0.0072\n",
      "Batch [50/5778] Loss: 0.0104\n",
      "Batch [60/5778] Loss: 0.0148\n",
      "Batch [70/5778] Loss: 0.0050\n",
      "Batch [80/5778] Loss: 0.0098\n",
      "Batch [90/5778] Loss: 0.0120\n",
      "Batch [100/5778] Loss: 0.0123\n",
      "Batch [110/5778] Loss: 0.0178\n",
      "Batch [120/5778] Loss: 0.0098\n",
      "Batch [130/5778] Loss: 0.0101\n",
      "Batch [140/5778] Loss: 0.0044\n",
      "Batch [150/5778] Loss: 0.0122\n",
      "Batch [160/5778] Loss: 0.0133\n",
      "Batch [170/5778] Loss: 0.0092\n",
      "Batch [180/5778] Loss: 0.0102\n",
      "Batch [190/5778] Loss: 0.0154\n",
      "Batch [200/5778] Loss: 0.0114\n",
      "Batch [210/5778] Loss: 0.0199\n",
      "Batch [220/5778] Loss: 0.0132\n",
      "Batch [230/5778] Loss: 0.0043\n",
      "Batch [240/5778] Loss: 0.0090\n",
      "Batch [250/5778] Loss: 0.0151\n",
      "Batch [260/5778] Loss: 0.0079\n",
      "Batch [270/5778] Loss: 0.0100\n",
      "Batch [280/5778] Loss: 0.0152\n",
      "Batch [290/5778] Loss: 0.0061\n",
      "Batch [300/5778] Loss: 0.0088\n",
      "Batch [310/5778] Loss: 0.0219\n",
      "Batch [320/5778] Loss: 0.0115\n",
      "Batch [330/5778] Loss: 0.0085\n",
      "Batch [340/5778] Loss: 0.0132\n",
      "Batch [350/5778] Loss: 0.0096\n",
      "Batch [360/5778] Loss: 0.0135\n",
      "Batch [370/5778] Loss: 0.0110\n",
      "Batch [380/5778] Loss: 0.0145\n",
      "Batch [390/5778] Loss: 0.0103\n",
      "Batch [400/5778] Loss: 0.0124\n",
      "Batch [410/5778] Loss: 0.0148\n",
      "Batch [420/5778] Loss: 0.0063\n",
      "Batch [430/5778] Loss: 0.0141\n",
      "Batch [440/5778] Loss: 0.0153\n",
      "Batch [450/5778] Loss: 0.0141\n",
      "Batch [460/5778] Loss: 0.0035\n",
      "Batch [470/5778] Loss: 0.0083\n",
      "Batch [480/5778] Loss: 0.0101\n",
      "Batch [490/5778] Loss: 0.0083\n",
      "Batch [500/5778] Loss: 0.0083\n",
      "Batch [510/5778] Loss: 0.0144\n",
      "Batch [520/5778] Loss: 0.0150\n",
      "Batch [530/5778] Loss: 0.0047\n",
      "Batch [540/5778] Loss: 0.0125\n",
      "Batch [550/5778] Loss: 0.0068\n",
      "Batch [560/5778] Loss: 0.0159\n",
      "Batch [570/5778] Loss: 0.0203\n",
      "Batch [580/5778] Loss: 0.0088\n",
      "Batch [590/5778] Loss: 0.0110\n",
      "Batch [600/5778] Loss: 0.0090\n",
      "Batch [610/5778] Loss: 0.0039\n",
      "Batch [620/5778] Loss: 0.0168\n",
      "Batch [630/5778] Loss: 0.0111\n",
      "Batch [640/5778] Loss: 0.0076\n",
      "Batch [650/5778] Loss: 0.0073\n",
      "Batch [660/5778] Loss: 0.0094\n",
      "Batch [670/5778] Loss: 0.0129\n",
      "Batch [680/5778] Loss: 0.0118\n",
      "Batch [690/5778] Loss: 0.0216\n",
      "Batch [700/5778] Loss: 0.0113\n",
      "Batch [710/5778] Loss: 0.0085\n",
      "Batch [720/5778] Loss: 0.0072\n",
      "Batch [730/5778] Loss: 0.0110\n",
      "Batch [740/5778] Loss: 0.0072\n",
      "Batch [750/5778] Loss: 0.0189\n",
      "Batch [760/5778] Loss: 0.0087\n",
      "Batch [770/5778] Loss: 0.0073\n",
      "Batch [780/5778] Loss: 0.0141\n",
      "Batch [790/5778] Loss: 0.0153\n",
      "Batch [800/5778] Loss: 0.0108\n",
      "Batch [810/5778] Loss: 0.0084\n",
      "Batch [820/5778] Loss: 0.0135\n",
      "Batch [830/5778] Loss: 0.0088\n",
      "Batch [840/5778] Loss: 0.0090\n",
      "Batch [850/5778] Loss: 0.0073\n",
      "Batch [860/5778] Loss: 0.0121\n",
      "Batch [870/5778] Loss: 0.0196\n",
      "Batch [880/5778] Loss: 0.0102\n",
      "Batch [890/5778] Loss: 0.0102\n",
      "Batch [900/5778] Loss: 0.0094\n",
      "Batch [910/5778] Loss: 0.0127\n",
      "Batch [920/5778] Loss: 0.0079\n",
      "Batch [930/5778] Loss: 0.0059\n",
      "Batch [940/5778] Loss: 0.0172\n",
      "Batch [950/5778] Loss: 0.0094\n",
      "Batch [960/5778] Loss: 0.0063\n",
      "Batch [970/5778] Loss: 0.0164\n",
      "Batch [980/5778] Loss: 0.0064\n",
      "Batch [990/5778] Loss: 0.0061\n",
      "Batch [1000/5778] Loss: 0.0085\n",
      "Batch [1010/5778] Loss: 0.0043\n",
      "Batch [1020/5778] Loss: 0.0097\n",
      "Batch [1030/5778] Loss: 0.0154\n",
      "Batch [1040/5778] Loss: 0.0125\n",
      "Batch [1050/5778] Loss: 0.0104\n",
      "Batch [1060/5778] Loss: 0.0110\n",
      "Batch [1070/5778] Loss: 0.0066\n",
      "Batch [1080/5778] Loss: 0.0032\n",
      "Batch [1090/5778] Loss: 0.0044\n",
      "Batch [1100/5778] Loss: 0.0071\n",
      "Batch [1110/5778] Loss: 0.0236\n",
      "Batch [1120/5778] Loss: 0.0090\n",
      "Batch [1130/5778] Loss: 0.0054\n",
      "Batch [1140/5778] Loss: 0.0114\n",
      "Batch [1150/5778] Loss: 0.0080\n",
      "Batch [1160/5778] Loss: 0.0050\n",
      "Batch [1170/5778] Loss: 0.0122\n",
      "Batch [1180/5778] Loss: 0.0153\n",
      "Batch [1190/5778] Loss: 0.0066\n",
      "Batch [1200/5778] Loss: 0.0098\n",
      "Batch [1210/5778] Loss: 0.0130\n",
      "Batch [1220/5778] Loss: 0.0071\n",
      "Batch [1230/5778] Loss: 0.0091\n",
      "Batch [1240/5778] Loss: 0.0075\n",
      "Batch [1250/5778] Loss: 0.0204\n",
      "Batch [1260/5778] Loss: 0.0107\n",
      "Batch [1270/5778] Loss: 0.0196\n",
      "Batch [1280/5778] Loss: 0.0132\n",
      "Batch [1290/5778] Loss: 0.0073\n",
      "Batch [1300/5778] Loss: 0.0040\n",
      "Batch [1310/5778] Loss: 0.0035\n",
      "Batch [1320/5778] Loss: 0.0131\n",
      "Batch [1330/5778] Loss: 0.0083\n",
      "Batch [1340/5778] Loss: 0.0154\n",
      "Batch [1350/5778] Loss: 0.0119\n",
      "Batch [1360/5778] Loss: 0.0123\n",
      "Batch [1370/5778] Loss: 0.0071\n",
      "Batch [1380/5778] Loss: 0.0134\n",
      "Batch [1390/5778] Loss: 0.0108\n",
      "Batch [1400/5778] Loss: 0.0103\n",
      "Batch [1410/5778] Loss: 0.0190\n",
      "Batch [1420/5778] Loss: 0.0051\n",
      "Batch [1430/5778] Loss: 0.0130\n",
      "Batch [1440/5778] Loss: 0.0052\n",
      "Batch [1450/5778] Loss: 0.0189\n",
      "Batch [1460/5778] Loss: 0.0064\n",
      "Batch [1470/5778] Loss: 0.0092\n",
      "Batch [1480/5778] Loss: 0.0044\n",
      "Batch [1490/5778] Loss: 0.0105\n",
      "Batch [1500/5778] Loss: 0.0132\n",
      "Batch [1510/5778] Loss: 0.0072\n",
      "Batch [1520/5778] Loss: 0.0060\n",
      "Batch [1530/5778] Loss: 0.0160\n",
      "Batch [1540/5778] Loss: 0.0193\n",
      "Batch [1550/5778] Loss: 0.0144\n",
      "Batch [1560/5778] Loss: 0.0118\n",
      "Batch [1570/5778] Loss: 0.0100\n",
      "Batch [1580/5778] Loss: 0.0241\n",
      "Batch [1590/5778] Loss: 0.0044\n",
      "Batch [1600/5778] Loss: 0.0052\n",
      "Batch [1610/5778] Loss: 0.0085\n",
      "Batch [1620/5778] Loss: 0.0174\n",
      "Batch [1630/5778] Loss: 0.0086\n",
      "Batch [1640/5778] Loss: 0.0085\n",
      "Batch [1650/5778] Loss: 0.0204\n",
      "Batch [1660/5778] Loss: 0.0108\n",
      "Batch [1670/5778] Loss: 0.0073\n",
      "Batch [1680/5778] Loss: 0.0043\n",
      "Batch [1690/5778] Loss: 0.0103\n",
      "Batch [1700/5778] Loss: 0.0090\n",
      "Batch [1710/5778] Loss: 0.0150\n",
      "Batch [1720/5778] Loss: 0.0093\n",
      "Batch [1730/5778] Loss: 0.0027\n",
      "Batch [1740/5778] Loss: 0.0121\n",
      "Batch [1750/5778] Loss: 0.0208\n",
      "Batch [1760/5778] Loss: 0.0101\n",
      "Batch [1770/5778] Loss: 0.0084\n",
      "Batch [1780/5778] Loss: 0.0055\n",
      "Batch [1790/5778] Loss: 0.0066\n",
      "Batch [1800/5778] Loss: 0.0099\n",
      "Batch [1810/5778] Loss: 0.0164\n",
      "Batch [1820/5778] Loss: 0.0097\n",
      "Batch [1830/5778] Loss: 0.0119\n",
      "Batch [1840/5778] Loss: 0.0223\n",
      "Batch [1850/5778] Loss: 0.0056\n",
      "Batch [1860/5778] Loss: 0.0134\n",
      "Batch [1870/5778] Loss: 0.0078\n",
      "Batch [1880/5778] Loss: 0.0151\n",
      "Batch [1890/5778] Loss: 0.0117\n",
      "Batch [1900/5778] Loss: 0.0111\n",
      "Batch [1910/5778] Loss: 0.0180\n",
      "Batch [1920/5778] Loss: 0.0129\n",
      "Batch [1930/5778] Loss: 0.0164\n",
      "Batch [1940/5778] Loss: 0.0154\n",
      "Batch [1950/5778] Loss: 0.0127\n",
      "Batch [1960/5778] Loss: 0.0116\n",
      "Batch [1970/5778] Loss: 0.0163\n",
      "Batch [1980/5778] Loss: 0.0163\n",
      "Batch [1990/5778] Loss: 0.0166\n",
      "Batch [2000/5778] Loss: 0.0146\n",
      "Batch [2010/5778] Loss: 0.0156\n",
      "Batch [2020/5778] Loss: 0.0183\n",
      "Batch [2030/5778] Loss: 0.0042\n",
      "Batch [2040/5778] Loss: 0.0115\n",
      "Batch [2050/5778] Loss: 0.0063\n",
      "Batch [2060/5778] Loss: 0.0193\n",
      "Batch [2070/5778] Loss: 0.0134\n",
      "Batch [2080/5778] Loss: 0.0168\n",
      "Batch [2090/5778] Loss: 0.0075\n",
      "Batch [2100/5778] Loss: 0.0152\n",
      "Batch [2110/5778] Loss: 0.0099\n",
      "Batch [2120/5778] Loss: 0.0085\n",
      "Batch [2130/5778] Loss: 0.0102\n",
      "Batch [2140/5778] Loss: 0.0210\n",
      "Batch [2150/5778] Loss: 0.0088\n",
      "Batch [2160/5778] Loss: 0.0115\n",
      "Batch [2170/5778] Loss: 0.0113\n",
      "Batch [2180/5778] Loss: 0.0097\n",
      "Batch [2190/5778] Loss: 0.0148\n",
      "Batch [2200/5778] Loss: 0.0085\n",
      "Batch [2210/5778] Loss: 0.0056\n",
      "Batch [2220/5778] Loss: 0.0091\n",
      "Batch [2230/5778] Loss: 0.0059\n",
      "Batch [2240/5778] Loss: 0.0079\n",
      "Batch [2250/5778] Loss: 0.0103\n",
      "Batch [2260/5778] Loss: 0.0116\n",
      "Batch [2270/5778] Loss: 0.0151\n",
      "Batch [2280/5778] Loss: 0.0078\n",
      "Batch [2290/5778] Loss: 0.0127\n",
      "Batch [2300/5778] Loss: 0.0064\n",
      "Batch [2310/5778] Loss: 0.0052\n",
      "Batch [2320/5778] Loss: 0.0059\n",
      "Batch [2330/5778] Loss: 0.0149\n",
      "Batch [2340/5778] Loss: 0.0082\n",
      "Batch [2350/5778] Loss: 0.0038\n",
      "Batch [2360/5778] Loss: 0.0167\n",
      "Batch [2370/5778] Loss: 0.0102\n",
      "Batch [2380/5778] Loss: 0.0076\n",
      "Batch [2390/5778] Loss: 0.0073\n",
      "Batch [2400/5778] Loss: 0.0091\n",
      "Batch [2410/5778] Loss: 0.0143\n",
      "Batch [2420/5778] Loss: 0.0151\n",
      "Batch [2430/5778] Loss: 0.0132\n",
      "Batch [2440/5778] Loss: 0.0075\n",
      "Batch [2450/5778] Loss: 0.0113\n",
      "Batch [2460/5778] Loss: 0.0111\n",
      "Batch [2470/5778] Loss: 0.0092\n",
      "Batch [2480/5778] Loss: 0.0076\n",
      "Batch [2490/5778] Loss: 0.0170\n",
      "Batch [2500/5778] Loss: 0.0188\n",
      "Batch [2510/5778] Loss: 0.0137\n",
      "Batch [2520/5778] Loss: 0.0099\n",
      "Batch [2530/5778] Loss: 0.0121\n",
      "Batch [2540/5778] Loss: 0.0093\n",
      "Batch [2550/5778] Loss: 0.0217\n",
      "Batch [2560/5778] Loss: 0.0121\n",
      "Batch [2570/5778] Loss: 0.0035\n",
      "Batch [2580/5778] Loss: 0.0066\n",
      "Batch [2590/5778] Loss: 0.0136\n",
      "Batch [2600/5778] Loss: 0.0174\n",
      "Batch [2610/5778] Loss: 0.0076\n",
      "Batch [2620/5778] Loss: 0.0120\n",
      "Batch [2630/5778] Loss: 0.0126\n",
      "Batch [2640/5778] Loss: 0.0198\n",
      "Batch [2650/5778] Loss: 0.0078\n",
      "Batch [2660/5778] Loss: 0.0121\n",
      "Batch [2670/5778] Loss: 0.0102\n",
      "Batch [2680/5778] Loss: 0.0159\n",
      "Batch [2690/5778] Loss: 0.0079\n",
      "Batch [2700/5778] Loss: 0.0095\n",
      "Batch [2710/5778] Loss: 0.0105\n",
      "Batch [2720/5778] Loss: 0.0117\n",
      "Batch [2730/5778] Loss: 0.0119\n",
      "Batch [2740/5778] Loss: 0.0068\n",
      "Batch [2750/5778] Loss: 0.0121\n",
      "Batch [2760/5778] Loss: 0.0141\n",
      "Batch [2770/5778] Loss: 0.0052\n",
      "Batch [2780/5778] Loss: 0.0093\n",
      "Batch [2790/5778] Loss: 0.0097\n",
      "Batch [2800/5778] Loss: 0.0203\n",
      "Batch [2810/5778] Loss: 0.0139\n",
      "Batch [2820/5778] Loss: 0.0122\n",
      "Batch [2830/5778] Loss: 0.0104\n",
      "Batch [2840/5778] Loss: 0.0100\n",
      "Batch [2850/5778] Loss: 0.0142\n",
      "Batch [2860/5778] Loss: 0.0103\n",
      "Batch [2870/5778] Loss: 0.0124\n",
      "Batch [2880/5778] Loss: 0.0149\n",
      "Batch [2890/5778] Loss: 0.0065\n",
      "Batch [2900/5778] Loss: 0.0136\n",
      "Batch [2910/5778] Loss: 0.0149\n",
      "Batch [2920/5778] Loss: 0.0074\n",
      "Batch [2930/5778] Loss: 0.0125\n",
      "Batch [2940/5778] Loss: 0.0083\n",
      "Batch [2950/5778] Loss: 0.0147\n",
      "Batch [2960/5778] Loss: 0.0123\n",
      "Batch [2970/5778] Loss: 0.0067\n",
      "Batch [2980/5778] Loss: 0.0070\n",
      "Batch [2990/5778] Loss: 0.0152\n",
      "Batch [3000/5778] Loss: 0.0073\n",
      "Batch [3010/5778] Loss: 0.0091\n",
      "Batch [3020/5778] Loss: 0.0082\n",
      "Batch [3030/5778] Loss: 0.0105\n",
      "Batch [3040/5778] Loss: 0.0091\n",
      "Batch [3050/5778] Loss: 0.0190\n",
      "Batch [3060/5778] Loss: 0.0108\n",
      "Batch [3070/5778] Loss: 0.0118\n",
      "Batch [3080/5778] Loss: 0.0094\n",
      "Batch [3090/5778] Loss: 0.0105\n",
      "Batch [3100/5778] Loss: 0.0094\n",
      "Batch [3110/5778] Loss: 0.0081\n",
      "Batch [3120/5778] Loss: 0.0170\n",
      "Batch [3130/5778] Loss: 0.0048\n",
      "Batch [3140/5778] Loss: 0.0118\n",
      "Batch [3150/5778] Loss: 0.0075\n",
      "Batch [3160/5778] Loss: 0.0075\n",
      "Batch [3170/5778] Loss: 0.0045\n",
      "Batch [3180/5778] Loss: 0.0093\n",
      "Batch [3190/5778] Loss: 0.0079\n",
      "Batch [3200/5778] Loss: 0.0190\n",
      "Batch [3210/5778] Loss: 0.0080\n",
      "Batch [3220/5778] Loss: 0.0095\n",
      "Batch [3230/5778] Loss: 0.0202\n",
      "Batch [3240/5778] Loss: 0.0047\n",
      "Batch [3250/5778] Loss: 0.0127\n",
      "Batch [3260/5778] Loss: 0.0094\n",
      "Batch [3270/5778] Loss: 0.0160\n",
      "Batch [3280/5778] Loss: 0.0108\n",
      "Batch [3290/5778] Loss: 0.0107\n",
      "Batch [3300/5778] Loss: 0.0097\n",
      "Batch [3310/5778] Loss: 0.0195\n",
      "Batch [3320/5778] Loss: 0.0164\n",
      "Batch [3330/5778] Loss: 0.0103\n",
      "Batch [3340/5778] Loss: 0.0052\n",
      "Batch [3350/5778] Loss: 0.0108\n",
      "Batch [3360/5778] Loss: 0.0219\n",
      "Batch [3370/5778] Loss: 0.0093\n",
      "Batch [3380/5778] Loss: 0.0129\n",
      "Batch [3390/5778] Loss: 0.0100\n",
      "Batch [3400/5778] Loss: 0.0150\n",
      "Batch [3410/5778] Loss: 0.0180\n",
      "Batch [3420/5778] Loss: 0.0104\n",
      "Batch [3430/5778] Loss: 0.0086\n",
      "Batch [3440/5778] Loss: 0.0073\n",
      "Batch [3450/5778] Loss: 0.0113\n",
      "Batch [3460/5778] Loss: 0.0056\n",
      "Batch [3470/5778] Loss: 0.0245\n",
      "Batch [3480/5778] Loss: 0.0157\n",
      "Batch [3490/5778] Loss: 0.0116\n",
      "Batch [3500/5778] Loss: 0.0053\n",
      "Batch [3510/5778] Loss: 0.0045\n",
      "Batch [3520/5778] Loss: 0.0102\n",
      "Batch [3530/5778] Loss: 0.0095\n",
      "Batch [3540/5778] Loss: 0.0129\n",
      "Batch [3550/5778] Loss: 0.0124\n",
      "Batch [3560/5778] Loss: 0.0132\n",
      "Batch [3570/5778] Loss: 0.0310\n",
      "Batch [3580/5778] Loss: 0.0255\n",
      "Batch [3590/5778] Loss: 0.0105\n",
      "Batch [3600/5778] Loss: 0.0045\n",
      "Batch [3610/5778] Loss: 0.0070\n",
      "Batch [3620/5778] Loss: 0.0170\n",
      "Batch [3630/5778] Loss: 0.0185\n",
      "Batch [3640/5778] Loss: 0.0062\n",
      "Batch [3650/5778] Loss: 0.0065\n",
      "Batch [3660/5778] Loss: 0.0075\n",
      "Batch [3670/5778] Loss: 0.0134\n",
      "Batch [3680/5778] Loss: 0.0153\n",
      "Batch [3690/5778] Loss: 0.0110\n",
      "Batch [3700/5778] Loss: 0.0068\n",
      "Batch [3710/5778] Loss: 0.0133\n",
      "Batch [3720/5778] Loss: 0.0083\n",
      "Batch [3730/5778] Loss: 0.0126\n",
      "Batch [3740/5778] Loss: 0.0206\n",
      "Batch [3750/5778] Loss: 0.0054\n",
      "Batch [3760/5778] Loss: 0.0075\n",
      "Batch [3770/5778] Loss: 0.0077\n",
      "Batch [3780/5778] Loss: 0.0085\n",
      "Batch [3790/5778] Loss: 0.0119\n",
      "Batch [3800/5778] Loss: 0.0198\n",
      "Batch [3810/5778] Loss: 0.0083\n",
      "Batch [3820/5778] Loss: 0.0059\n",
      "Batch [3830/5778] Loss: 0.0137\n",
      "Batch [3840/5778] Loss: 0.0099\n",
      "Batch [3850/5778] Loss: 0.0061\n",
      "Batch [3860/5778] Loss: 0.0066\n",
      "Batch [3870/5778] Loss: 0.0167\n",
      "Batch [3880/5778] Loss: 0.0139\n",
      "Batch [3890/5778] Loss: 0.0100\n",
      "Batch [3900/5778] Loss: 0.0113\n",
      "Batch [3910/5778] Loss: 0.0124\n",
      "Batch [3920/5778] Loss: 0.0196\n",
      "Batch [3930/5778] Loss: 0.0153\n",
      "Batch [3940/5778] Loss: 0.0069\n",
      "Batch [3950/5778] Loss: 0.0087\n",
      "Batch [3960/5778] Loss: 0.0118\n",
      "Batch [3970/5778] Loss: 0.0086\n",
      "Batch [3980/5778] Loss: 0.0157\n",
      "Batch [3990/5778] Loss: 0.0067\n",
      "Batch [4000/5778] Loss: 0.0080\n",
      "Batch [4010/5778] Loss: 0.0072\n",
      "Batch [4020/5778] Loss: 0.0218\n",
      "Batch [4030/5778] Loss: 0.0108\n",
      "Batch [4040/5778] Loss: 0.0132\n",
      "Batch [4050/5778] Loss: 0.0147\n",
      "Batch [4060/5778] Loss: 0.0037\n",
      "Batch [4070/5778] Loss: 0.0122\n",
      "Batch [4080/5778] Loss: 0.0061\n",
      "Batch [4090/5778] Loss: 0.0057\n",
      "Batch [4100/5778] Loss: 0.0050\n",
      "Batch [4110/5778] Loss: 0.0122\n",
      "Batch [4120/5778] Loss: 0.0139\n",
      "Batch [4130/5778] Loss: 0.0061\n",
      "Batch [4140/5778] Loss: 0.0077\n",
      "Batch [4150/5778] Loss: 0.0099\n",
      "Batch [4160/5778] Loss: 0.0062\n",
      "Batch [4170/5778] Loss: 0.0093\n",
      "Batch [4180/5778] Loss: 0.0076\n",
      "Batch [4190/5778] Loss: 0.0101\n",
      "Batch [4200/5778] Loss: 0.0073\n",
      "Batch [4210/5778] Loss: 0.0072\n",
      "Batch [4220/5778] Loss: 0.0126\n",
      "Batch [4230/5778] Loss: 0.0087\n",
      "Batch [4240/5778] Loss: 0.0113\n",
      "Batch [4250/5778] Loss: 0.0123\n",
      "Batch [4260/5778] Loss: 0.0062\n",
      "Batch [4270/5778] Loss: 0.0065\n",
      "Batch [4280/5778] Loss: 0.0157\n",
      "Batch [4290/5778] Loss: 0.0087\n",
      "Batch [4300/5778] Loss: 0.0045\n",
      "Batch [4310/5778] Loss: 0.0060\n",
      "Batch [4320/5778] Loss: 0.0091\n",
      "Batch [4330/5778] Loss: 0.0063\n",
      "Batch [4340/5778] Loss: 0.0129\n",
      "Batch [4350/5778] Loss: 0.0068\n",
      "Batch [4360/5778] Loss: 0.0063\n",
      "Batch [4370/5778] Loss: 0.0115\n",
      "Batch [4380/5778] Loss: 0.0202\n",
      "Batch [4390/5778] Loss: 0.0068\n",
      "Batch [4400/5778] Loss: 0.0084\n",
      "Batch [4410/5778] Loss: 0.0172\n",
      "Batch [4420/5778] Loss: 0.0085\n",
      "Batch [4430/5778] Loss: 0.0143\n",
      "Batch [4440/5778] Loss: 0.0171\n",
      "Batch [4450/5778] Loss: 0.0108\n",
      "Batch [4460/5778] Loss: 0.0056\n",
      "Batch [4470/5778] Loss: 0.0086\n",
      "Batch [4480/5778] Loss: 0.0240\n",
      "Batch [4490/5778] Loss: 0.0061\n",
      "Batch [4500/5778] Loss: 0.0119\n",
      "Batch [4510/5778] Loss: 0.0040\n",
      "Batch [4520/5778] Loss: 0.0121\n",
      "Batch [4530/5778] Loss: 0.0077\n",
      "Batch [4540/5778] Loss: 0.0212\n",
      "Batch [4550/5778] Loss: 0.0234\n",
      "Batch [4560/5778] Loss: 0.0085\n",
      "Batch [4570/5778] Loss: 0.0198\n",
      "Batch [4580/5778] Loss: 0.0172\n",
      "Batch [4590/5778] Loss: 0.0208\n",
      "Batch [4600/5778] Loss: 0.0070\n",
      "Batch [4610/5778] Loss: 0.0062\n",
      "Batch [4620/5778] Loss: 0.0060\n",
      "Batch [4630/5778] Loss: 0.0212\n",
      "Batch [4640/5778] Loss: 0.0142\n",
      "Batch [4650/5778] Loss: 0.0166\n",
      "Batch [4660/5778] Loss: 0.0078\n",
      "Batch [4670/5778] Loss: 0.0108\n",
      "Batch [4680/5778] Loss: 0.0068\n",
      "Batch [4690/5778] Loss: 0.0124\n",
      "Batch [4700/5778] Loss: 0.0047\n",
      "Batch [4710/5778] Loss: 0.0150\n",
      "Batch [4720/5778] Loss: 0.0204\n",
      "Batch [4730/5778] Loss: 0.0306\n",
      "Batch [4740/5778] Loss: 0.0122\n",
      "Batch [4750/5778] Loss: 0.0117\n",
      "Batch [4760/5778] Loss: 0.0106\n",
      "Batch [4770/5778] Loss: 0.0086\n",
      "Batch [4780/5778] Loss: 0.0087\n",
      "Batch [4790/5778] Loss: 0.0048\n",
      "Batch [4800/5778] Loss: 0.0087\n",
      "Batch [4810/5778] Loss: 0.0194\n",
      "Batch [4820/5778] Loss: 0.0077\n",
      "Batch [4830/5778] Loss: 0.0080\n",
      "Batch [4840/5778] Loss: 0.0134\n",
      "Batch [4850/5778] Loss: 0.0064\n",
      "Batch [4860/5778] Loss: 0.0052\n",
      "Batch [4870/5778] Loss: 0.0056\n",
      "Batch [4880/5778] Loss: 0.0092\n",
      "Batch [4890/5778] Loss: 0.0162\n",
      "Batch [4900/5778] Loss: 0.0113\n",
      "Batch [4910/5778] Loss: 0.0109\n",
      "Batch [4920/5778] Loss: 0.0107\n",
      "Batch [4930/5778] Loss: 0.0027\n",
      "Batch [4940/5778] Loss: 0.0115\n",
      "Batch [4950/5778] Loss: 0.0042\n",
      "Batch [4960/5778] Loss: 0.0102\n",
      "Batch [4970/5778] Loss: 0.0102\n",
      "Batch [4980/5778] Loss: 0.0080\n",
      "Batch [4990/5778] Loss: 0.0046\n",
      "Batch [5000/5778] Loss: 0.0078\n",
      "Batch [5010/5778] Loss: 0.0116\n",
      "Batch [5020/5778] Loss: 0.0085\n",
      "Batch [5030/5778] Loss: 0.0122\n",
      "Batch [5040/5778] Loss: 0.0160\n",
      "Batch [5050/5778] Loss: 0.0125\n",
      "Batch [5060/5778] Loss: 0.0061\n",
      "Batch [5070/5778] Loss: 0.0119\n",
      "Batch [5080/5778] Loss: 0.0122\n",
      "Batch [5090/5778] Loss: 0.0142\n",
      "Batch [5100/5778] Loss: 0.0056\n",
      "Batch [5110/5778] Loss: 0.0100\n",
      "Batch [5120/5778] Loss: 0.0074\n",
      "Batch [5130/5778] Loss: 0.0128\n",
      "Batch [5140/5778] Loss: 0.0110\n",
      "Batch [5150/5778] Loss: 0.0075\n",
      "Batch [5160/5778] Loss: 0.0110\n",
      "Batch [5170/5778] Loss: 0.0057\n",
      "Batch [5180/5778] Loss: 0.0072\n",
      "Batch [5190/5778] Loss: 0.0057\n",
      "Batch [5200/5778] Loss: 0.0045\n",
      "Batch [5210/5778] Loss: 0.0091\n",
      "Batch [5220/5778] Loss: 0.0105\n",
      "Batch [5230/5778] Loss: 0.0100\n",
      "Batch [5240/5778] Loss: 0.0075\n",
      "Batch [5250/5778] Loss: 0.0146\n",
      "Batch [5260/5778] Loss: 0.0067\n",
      "Batch [5270/5778] Loss: 0.0140\n",
      "Batch [5280/5778] Loss: 0.0075\n",
      "Batch [5290/5778] Loss: 0.0065\n",
      "Batch [5300/5778] Loss: 0.0082\n",
      "Batch [5310/5778] Loss: 0.0078\n",
      "Batch [5320/5778] Loss: 0.0096\n",
      "Batch [5330/5778] Loss: 0.0092\n",
      "Batch [5340/5778] Loss: 0.0121\n",
      "Batch [5350/5778] Loss: 0.0083\n",
      "Batch [5360/5778] Loss: 0.0118\n",
      "Batch [5370/5778] Loss: 0.0063\n",
      "Batch [5380/5778] Loss: 0.0145\n",
      "Batch [5390/5778] Loss: 0.0106\n",
      "Batch [5400/5778] Loss: 0.0087\n",
      "Batch [5410/5778] Loss: 0.0097\n",
      "Batch [5420/5778] Loss: 0.0131\n",
      "Batch [5430/5778] Loss: 0.0128\n",
      "Batch [5440/5778] Loss: 0.0202\n",
      "Batch [5450/5778] Loss: 0.0063\n",
      "Batch [5460/5778] Loss: 0.0094\n",
      "Batch [5470/5778] Loss: 0.0204\n",
      "Batch [5480/5778] Loss: 0.0146\n",
      "Batch [5490/5778] Loss: 0.0112\n",
      "Batch [5500/5778] Loss: 0.0084\n",
      "Batch [5510/5778] Loss: 0.0155\n",
      "Batch [5520/5778] Loss: 0.0224\n",
      "Batch [5530/5778] Loss: 0.0194\n",
      "Batch [5540/5778] Loss: 0.0108\n",
      "Batch [5550/5778] Loss: 0.0129\n",
      "Batch [5560/5778] Loss: 0.0114\n",
      "Batch [5570/5778] Loss: 0.0067\n",
      "Batch [5580/5778] Loss: 0.0151\n",
      "Batch [5590/5778] Loss: 0.0100\n",
      "Batch [5600/5778] Loss: 0.0104\n",
      "Batch [5610/5778] Loss: 0.0067\n",
      "Batch [5620/5778] Loss: 0.0059\n",
      "Batch [5630/5778] Loss: 0.0101\n",
      "Batch [5640/5778] Loss: 0.0023\n",
      "Batch [5650/5778] Loss: 0.0103\n",
      "Batch [5660/5778] Loss: 0.0133\n",
      "Batch [5670/5778] Loss: 0.0042\n",
      "Batch [5680/5778] Loss: 0.0152\n",
      "Batch [5690/5778] Loss: 0.0083\n",
      "Batch [5700/5778] Loss: 0.0153\n",
      "Batch [5710/5778] Loss: 0.0128\n",
      "Batch [5720/5778] Loss: 0.0036\n",
      "Batch [5730/5778] Loss: 0.0053\n",
      "Batch [5740/5778] Loss: 0.0083\n",
      "Batch [5750/5778] Loss: 0.0189\n",
      "Batch [5760/5778] Loss: 0.0055\n",
      "Batch [5770/5778] Loss: 0.0059\n",
      "Epoch [3/5] Average Loss: 0.0114\n",
      "Epoch [3/5] Validation Loss: 0.0101 Accuracy: 99.60%\n",
      "Epoch [4/5]\n",
      "Batch [0/5778] Loss: 0.0109\n",
      "Batch [10/5778] Loss: 0.0042\n",
      "Batch [20/5778] Loss: 0.0083\n",
      "Batch [30/5778] Loss: 0.0093\n",
      "Batch [40/5778] Loss: 0.0071\n",
      "Batch [50/5778] Loss: 0.0218\n",
      "Batch [60/5778] Loss: 0.0119\n",
      "Batch [70/5778] Loss: 0.0085\n",
      "Batch [80/5778] Loss: 0.0149\n",
      "Batch [90/5778] Loss: 0.0092\n",
      "Batch [100/5778] Loss: 0.0156\n",
      "Batch [110/5778] Loss: 0.0051\n",
      "Batch [120/5778] Loss: 0.0098\n",
      "Batch [130/5778] Loss: 0.0103\n",
      "Batch [140/5778] Loss: 0.0106\n",
      "Batch [150/5778] Loss: 0.0118\n",
      "Batch [160/5778] Loss: 0.0084\n",
      "Batch [170/5778] Loss: 0.0041\n",
      "Batch [180/5778] Loss: 0.0060\n",
      "Batch [190/5778] Loss: 0.0076\n",
      "Batch [200/5778] Loss: 0.0138\n",
      "Batch [210/5778] Loss: 0.0096\n",
      "Batch [220/5778] Loss: 0.0141\n",
      "Batch [230/5778] Loss: 0.0125\n",
      "Batch [240/5778] Loss: 0.0083\n",
      "Batch [250/5778] Loss: 0.0092\n",
      "Batch [260/5778] Loss: 0.0102\n",
      "Batch [270/5778] Loss: 0.0055\n",
      "Batch [280/5778] Loss: 0.0115\n",
      "Batch [290/5778] Loss: 0.0144\n",
      "Batch [300/5778] Loss: 0.0048\n",
      "Batch [310/5778] Loss: 0.0070\n",
      "Batch [320/5778] Loss: 0.0108\n",
      "Batch [330/5778] Loss: 0.0097\n",
      "Batch [340/5778] Loss: 0.0100\n",
      "Batch [350/5778] Loss: 0.0265\n",
      "Batch [360/5778] Loss: 0.0047\n",
      "Batch [370/5778] Loss: 0.0170\n",
      "Batch [380/5778] Loss: 0.0223\n",
      "Batch [390/5778] Loss: 0.0078\n",
      "Batch [400/5778] Loss: 0.0062\n",
      "Batch [410/5778] Loss: 0.0063\n",
      "Batch [420/5778] Loss: 0.0083\n",
      "Batch [430/5778] Loss: 0.0135\n",
      "Batch [440/5778] Loss: 0.0176\n",
      "Batch [450/5778] Loss: 0.0075\n",
      "Batch [460/5778] Loss: 0.0112\n",
      "Batch [470/5778] Loss: 0.0088\n",
      "Batch [480/5778] Loss: 0.0120\n",
      "Batch [490/5778] Loss: 0.0107\n",
      "Batch [500/5778] Loss: 0.0187\n",
      "Batch [510/5778] Loss: 0.0063\n",
      "Batch [520/5778] Loss: 0.0235\n",
      "Batch [530/5778] Loss: 0.0143\n",
      "Batch [540/5778] Loss: 0.0072\n",
      "Batch [550/5778] Loss: 0.0094\n",
      "Batch [560/5778] Loss: 0.0099\n",
      "Batch [570/5778] Loss: 0.0084\n",
      "Batch [580/5778] Loss: 0.0072\n",
      "Batch [590/5778] Loss: 0.0109\n",
      "Batch [600/5778] Loss: 0.0046\n",
      "Batch [610/5778] Loss: 0.0049\n",
      "Batch [620/5778] Loss: 0.0145\n",
      "Batch [630/5778] Loss: 0.0047\n",
      "Batch [640/5778] Loss: 0.0087\n",
      "Batch [650/5778] Loss: 0.0108\n",
      "Batch [660/5778] Loss: 0.0049\n",
      "Batch [670/5778] Loss: 0.0088\n",
      "Batch [680/5778] Loss: 0.0163\n",
      "Batch [690/5778] Loss: 0.0198\n",
      "Batch [700/5778] Loss: 0.0120\n",
      "Batch [710/5778] Loss: 0.0123\n",
      "Batch [720/5778] Loss: 0.0085\n",
      "Batch [730/5778] Loss: 0.0170\n",
      "Batch [740/5778] Loss: 0.0103\n",
      "Batch [750/5778] Loss: 0.0089\n",
      "Batch [760/5778] Loss: 0.0068\n",
      "Batch [770/5778] Loss: 0.0061\n",
      "Batch [780/5778] Loss: 0.0080\n",
      "Batch [790/5778] Loss: 0.0091\n",
      "Batch [800/5778] Loss: 0.0062\n",
      "Batch [810/5778] Loss: 0.0042\n",
      "Batch [820/5778] Loss: 0.0139\n",
      "Batch [830/5778] Loss: 0.0199\n",
      "Batch [840/5778] Loss: 0.0053\n",
      "Batch [850/5778] Loss: 0.0167\n",
      "Batch [860/5778] Loss: 0.0067\n",
      "Batch [870/5778] Loss: 0.0086\n",
      "Batch [880/5778] Loss: 0.0091\n",
      "Batch [890/5778] Loss: 0.0147\n",
      "Batch [900/5778] Loss: 0.0152\n",
      "Batch [910/5778] Loss: 0.0106\n",
      "Batch [920/5778] Loss: 0.0047\n",
      "Batch [930/5778] Loss: 0.0097\n",
      "Batch [940/5778] Loss: 0.0092\n",
      "Batch [950/5778] Loss: 0.0052\n",
      "Batch [960/5778] Loss: 0.0089\n",
      "Batch [970/5778] Loss: 0.0028\n",
      "Batch [980/5778] Loss: 0.0169\n",
      "Batch [990/5778] Loss: 0.0109\n",
      "Batch [1000/5778] Loss: 0.0083\n",
      "Batch [1010/5778] Loss: 0.0066\n",
      "Batch [1020/5778] Loss: 0.0096\n",
      "Batch [1030/5778] Loss: 0.0054\n",
      "Batch [1040/5778] Loss: 0.0091\n",
      "Batch [1050/5778] Loss: 0.0132\n",
      "Batch [1060/5778] Loss: 0.0056\n",
      "Batch [1070/5778] Loss: 0.0156\n",
      "Batch [1080/5778] Loss: 0.0091\n",
      "Batch [1090/5778] Loss: 0.0085\n",
      "Batch [1100/5778] Loss: 0.0024\n",
      "Batch [1110/5778] Loss: 0.0076\n",
      "Batch [1120/5778] Loss: 0.0175\n",
      "Batch [1130/5778] Loss: 0.0080\n",
      "Batch [1140/5778] Loss: 0.0164\n",
      "Batch [1150/5778] Loss: 0.0115\n",
      "Batch [1160/5778] Loss: 0.0168\n",
      "Batch [1170/5778] Loss: 0.0059\n",
      "Batch [1180/5778] Loss: 0.0078\n",
      "Batch [1190/5778] Loss: 0.0142\n",
      "Batch [1200/5778] Loss: 0.0105\n",
      "Batch [1210/5778] Loss: 0.0110\n",
      "Batch [1220/5778] Loss: 0.0103\n",
      "Batch [1230/5778] Loss: 0.0121\n",
      "Batch [1240/5778] Loss: 0.0089\n",
      "Batch [1250/5778] Loss: 0.0138\n",
      "Batch [1260/5778] Loss: 0.0092\n",
      "Batch [1270/5778] Loss: 0.0104\n",
      "Batch [1280/5778] Loss: 0.0143\n",
      "Batch [1290/5778] Loss: 0.0078\n",
      "Batch [1300/5778] Loss: 0.0149\n",
      "Batch [1310/5778] Loss: 0.0039\n",
      "Batch [1320/5778] Loss: 0.0251\n",
      "Batch [1330/5778] Loss: 0.0053\n",
      "Batch [1340/5778] Loss: 0.0096\n",
      "Batch [1350/5778] Loss: 0.0099\n",
      "Batch [1360/5778] Loss: 0.0134\n",
      "Batch [1370/5778] Loss: 0.0087\n",
      "Batch [1380/5778] Loss: 0.0075\n",
      "Batch [1390/5778] Loss: 0.0095\n",
      "Batch [1400/5778] Loss: 0.0060\n",
      "Batch [1410/5778] Loss: 0.0072\n",
      "Batch [1420/5778] Loss: 0.0081\n",
      "Batch [1430/5778] Loss: 0.0032\n",
      "Batch [1440/5778] Loss: 0.0069\n",
      "Batch [1450/5778] Loss: 0.0047\n",
      "Batch [1460/5778] Loss: 0.0066\n",
      "Batch [1470/5778] Loss: 0.0099\n",
      "Batch [1480/5778] Loss: 0.0047\n",
      "Batch [1490/5778] Loss: 0.0120\n",
      "Batch [1500/5778] Loss: 0.0058\n",
      "Batch [1510/5778] Loss: 0.0161\n",
      "Batch [1520/5778] Loss: 0.0030\n",
      "Batch [1530/5778] Loss: 0.0066\n",
      "Batch [1540/5778] Loss: 0.0056\n",
      "Batch [1550/5778] Loss: 0.0125\n",
      "Batch [1560/5778] Loss: 0.0117\n",
      "Batch [1570/5778] Loss: 0.0069\n",
      "Batch [1580/5778] Loss: 0.0119\n",
      "Batch [1590/5778] Loss: 0.0158\n",
      "Batch [1600/5778] Loss: 0.0231\n",
      "Batch [1610/5778] Loss: 0.0111\n",
      "Batch [1620/5778] Loss: 0.0042\n",
      "Batch [1630/5778] Loss: 0.0046\n",
      "Batch [1640/5778] Loss: 0.0093\n",
      "Batch [1650/5778] Loss: 0.0091\n",
      "Batch [1660/5778] Loss: 0.0094\n",
      "Batch [1670/5778] Loss: 0.0153\n",
      "Batch [1680/5778] Loss: 0.0069\n",
      "Batch [1690/5778] Loss: 0.0072\n",
      "Batch [1700/5778] Loss: 0.0115\n",
      "Batch [1710/5778] Loss: 0.0073\n",
      "Batch [1720/5778] Loss: 0.0087\n",
      "Batch [1730/5778] Loss: 0.0067\n",
      "Batch [1740/5778] Loss: 0.0068\n",
      "Batch [1750/5778] Loss: 0.0073\n",
      "Batch [1760/5778] Loss: 0.0031\n",
      "Batch [1770/5778] Loss: 0.0041\n",
      "Batch [1780/5778] Loss: 0.0122\n",
      "Batch [1790/5778] Loss: 0.0131\n",
      "Batch [1800/5778] Loss: 0.0052\n",
      "Batch [1810/5778] Loss: 0.0081\n",
      "Batch [1820/5778] Loss: 0.0087\n",
      "Batch [1830/5778] Loss: 0.0039\n",
      "Batch [1840/5778] Loss: 0.0150\n",
      "Batch [1850/5778] Loss: 0.0084\n",
      "Batch [1860/5778] Loss: 0.0068\n",
      "Batch [1870/5778] Loss: 0.0059\n",
      "Batch [1880/5778] Loss: 0.0022\n",
      "Batch [1890/5778] Loss: 0.0082\n",
      "Batch [1900/5778] Loss: 0.0044\n",
      "Batch [1910/5778] Loss: 0.0066\n",
      "Batch [1920/5778] Loss: 0.0080\n",
      "Batch [1930/5778] Loss: 0.0167\n",
      "Batch [1940/5778] Loss: 0.0110\n",
      "Batch [1950/5778] Loss: 0.0152\n",
      "Batch [1960/5778] Loss: 0.0147\n",
      "Batch [1970/5778] Loss: 0.0090\n",
      "Batch [1980/5778] Loss: 0.0055\n",
      "Batch [1990/5778] Loss: 0.0083\n",
      "Batch [2000/5778] Loss: 0.0147\n",
      "Batch [2010/5778] Loss: 0.0100\n",
      "Batch [2020/5778] Loss: 0.0121\n",
      "Batch [2030/5778] Loss: 0.0012\n",
      "Batch [2040/5778] Loss: 0.0090\n",
      "Batch [2050/5778] Loss: 0.0052\n",
      "Batch [2060/5778] Loss: 0.0102\n",
      "Batch [2070/5778] Loss: 0.0101\n",
      "Batch [2080/5778] Loss: 0.0080\n",
      "Batch [2090/5778] Loss: 0.0109\n",
      "Batch [2100/5778] Loss: 0.0090\n",
      "Batch [2110/5778] Loss: 0.0093\n",
      "Batch [2120/5778] Loss: 0.0064\n",
      "Batch [2130/5778] Loss: 0.0078\n",
      "Batch [2140/5778] Loss: 0.0087\n",
      "Batch [2150/5778] Loss: 0.0151\n",
      "Batch [2160/5778] Loss: 0.0090\n",
      "Batch [2170/5778] Loss: 0.0081\n",
      "Batch [2180/5778] Loss: 0.0142\n",
      "Batch [2190/5778] Loss: 0.0081\n",
      "Batch [2200/5778] Loss: 0.0055\n",
      "Batch [2210/5778] Loss: 0.0040\n",
      "Batch [2220/5778] Loss: 0.0077\n",
      "Batch [2230/5778] Loss: 0.0027\n",
      "Batch [2240/5778] Loss: 0.0055\n",
      "Batch [2250/5778] Loss: 0.0018\n",
      "Batch [2260/5778] Loss: 0.0093\n",
      "Batch [2270/5778] Loss: 0.0084\n",
      "Batch [2280/5778] Loss: 0.0167\n",
      "Batch [2290/5778] Loss: 0.0068\n",
      "Batch [2300/5778] Loss: 0.0053\n",
      "Batch [2310/5778] Loss: 0.0088\n",
      "Batch [2320/5778] Loss: 0.0108\n",
      "Batch [2330/5778] Loss: 0.0128\n",
      "Batch [2340/5778] Loss: 0.0076\n",
      "Batch [2350/5778] Loss: 0.0100\n",
      "Batch [2360/5778] Loss: 0.0087\n",
      "Batch [2370/5778] Loss: 0.0084\n",
      "Batch [2380/5778] Loss: 0.0116\n",
      "Batch [2390/5778] Loss: 0.0116\n",
      "Batch [2400/5778] Loss: 0.0179\n",
      "Batch [2410/5778] Loss: 0.0069\n",
      "Batch [2420/5778] Loss: 0.0155\n",
      "Batch [2430/5778] Loss: 0.0051\n",
      "Batch [2440/5778] Loss: 0.0111\n",
      "Batch [2450/5778] Loss: 0.0070\n",
      "Batch [2460/5778] Loss: 0.0068\n",
      "Batch [2470/5778] Loss: 0.0105\n",
      "Batch [2480/5778] Loss: 0.0043\n",
      "Batch [2490/5778] Loss: 0.0111\n",
      "Batch [2500/5778] Loss: 0.0131\n",
      "Batch [2510/5778] Loss: 0.0114\n",
      "Batch [2520/5778] Loss: 0.0040\n",
      "Batch [2530/5778] Loss: 0.0063\n",
      "Batch [2540/5778] Loss: 0.0089\n",
      "Batch [2550/5778] Loss: 0.0056\n",
      "Batch [2560/5778] Loss: 0.0040\n",
      "Batch [2570/5778] Loss: 0.0102\n",
      "Batch [2580/5778] Loss: 0.0089\n",
      "Batch [2590/5778] Loss: 0.0075\n",
      "Batch [2600/5778] Loss: 0.0075\n",
      "Batch [2610/5778] Loss: 0.0075\n",
      "Batch [2620/5778] Loss: 0.0085\n",
      "Batch [2630/5778] Loss: 0.0134\n",
      "Batch [2640/5778] Loss: 0.0086\n",
      "Batch [2650/5778] Loss: 0.0112\n",
      "Batch [2660/5778] Loss: 0.0138\n",
      "Batch [2670/5778] Loss: 0.0092\n",
      "Batch [2680/5778] Loss: 0.0099\n",
      "Batch [2690/5778] Loss: 0.0092\n",
      "Batch [2700/5778] Loss: 0.0141\n",
      "Batch [2710/5778] Loss: 0.0087\n",
      "Batch [2720/5778] Loss: 0.0134\n",
      "Batch [2730/5778] Loss: 0.0160\n",
      "Batch [2740/5778] Loss: 0.0142\n",
      "Batch [2750/5778] Loss: 0.0101\n",
      "Batch [2760/5778] Loss: 0.0133\n",
      "Batch [2770/5778] Loss: 0.0055\n",
      "Batch [2780/5778] Loss: 0.0092\n",
      "Batch [2790/5778] Loss: 0.0098\n",
      "Batch [2800/5778] Loss: 0.0051\n",
      "Batch [2810/5778] Loss: 0.0062\n",
      "Batch [2820/5778] Loss: 0.0127\n",
      "Batch [2830/5778] Loss: 0.0137\n",
      "Batch [2840/5778] Loss: 0.0061\n",
      "Batch [2850/5778] Loss: 0.0075\n",
      "Batch [2860/5778] Loss: 0.0067\n",
      "Batch [2870/5778] Loss: 0.0084\n",
      "Batch [2880/5778] Loss: 0.0099\n",
      "Batch [2890/5778] Loss: 0.0112\n",
      "Batch [2900/5778] Loss: 0.0066\n",
      "Batch [2910/5778] Loss: 0.0102\n",
      "Batch [2920/5778] Loss: 0.0150\n",
      "Batch [2930/5778] Loss: 0.0050\n",
      "Batch [2940/5778] Loss: 0.0065\n",
      "Batch [2950/5778] Loss: 0.0092\n",
      "Batch [2960/5778] Loss: 0.0143\n",
      "Batch [2970/5778] Loss: 0.0062\n",
      "Batch [2980/5778] Loss: 0.0135\n",
      "Batch [2990/5778] Loss: 0.0110\n",
      "Batch [3000/5778] Loss: 0.0095\n",
      "Batch [3010/5778] Loss: 0.0039\n",
      "Batch [3020/5778] Loss: 0.0147\n",
      "Batch [3030/5778] Loss: 0.0106\n",
      "Batch [3040/5778] Loss: 0.0176\n",
      "Batch [3050/5778] Loss: 0.0155\n",
      "Batch [3060/5778] Loss: 0.0196\n",
      "Batch [3070/5778] Loss: 0.0066\n",
      "Batch [3080/5778] Loss: 0.0045\n",
      "Batch [3090/5778] Loss: 0.0195\n",
      "Batch [3100/5778] Loss: 0.0091\n",
      "Batch [3110/5778] Loss: 0.0118\n",
      "Batch [3120/5778] Loss: 0.0082\n",
      "Batch [3130/5778] Loss: 0.0096\n",
      "Batch [3140/5778] Loss: 0.0106\n",
      "Batch [3150/5778] Loss: 0.0070\n",
      "Batch [3160/5778] Loss: 0.0080\n",
      "Batch [3170/5778] Loss: 0.0065\n",
      "Batch [3180/5778] Loss: 0.0116\n",
      "Batch [3190/5778] Loss: 0.0184\n",
      "Batch [3200/5778] Loss: 0.0130\n",
      "Batch [3210/5778] Loss: 0.0127\n",
      "Batch [3220/5778] Loss: 0.0144\n",
      "Batch [3230/5778] Loss: 0.0072\n",
      "Batch [3240/5778] Loss: 0.0101\n",
      "Batch [3250/5778] Loss: 0.0160\n",
      "Batch [3260/5778] Loss: 0.0103\n",
      "Batch [3270/5778] Loss: 0.0116\n",
      "Batch [3280/5778] Loss: 0.0167\n",
      "Batch [3290/5778] Loss: 0.0057\n",
      "Batch [3300/5778] Loss: 0.0057\n",
      "Batch [3310/5778] Loss: 0.0054\n",
      "Batch [3320/5778] Loss: 0.0078\n",
      "Batch [3330/5778] Loss: 0.0072\n",
      "Batch [3340/5778] Loss: 0.0053\n",
      "Batch [3350/5778] Loss: 0.0073\n",
      "Batch [3360/5778] Loss: 0.0073\n",
      "Batch [3370/5778] Loss: 0.0112\n",
      "Batch [3380/5778] Loss: 0.0189\n",
      "Batch [3390/5778] Loss: 0.0050\n",
      "Batch [3400/5778] Loss: 0.0080\n",
      "Batch [3410/5778] Loss: 0.0072\n",
      "Batch [3420/5778] Loss: 0.0076\n",
      "Batch [3430/5778] Loss: 0.0103\n",
      "Batch [3440/5778] Loss: 0.0168\n",
      "Batch [3450/5778] Loss: 0.0104\n",
      "Batch [3460/5778] Loss: 0.0045\n",
      "Batch [3470/5778] Loss: 0.0094\n",
      "Batch [3480/5778] Loss: 0.0096\n",
      "Batch [3490/5778] Loss: 0.0138\n",
      "Batch [3500/5778] Loss: 0.0154\n",
      "Batch [3510/5778] Loss: 0.0059\n",
      "Batch [3520/5778] Loss: 0.0201\n",
      "Batch [3530/5778] Loss: 0.0043\n",
      "Batch [3540/5778] Loss: 0.0111\n",
      "Batch [3550/5778] Loss: 0.0098\n",
      "Batch [3560/5778] Loss: 0.0081\n",
      "Batch [3570/5778] Loss: 0.0088\n",
      "Batch [3580/5778] Loss: 0.0070\n",
      "Batch [3590/5778] Loss: 0.0093\n",
      "Batch [3600/5778] Loss: 0.0158\n",
      "Batch [3610/5778] Loss: 0.0064\n",
      "Batch [3620/5778] Loss: 0.0132\n",
      "Batch [3630/5778] Loss: 0.0107\n",
      "Batch [3640/5778] Loss: 0.0066\n",
      "Batch [3650/5778] Loss: 0.0114\n",
      "Batch [3660/5778] Loss: 0.0038\n",
      "Batch [3670/5778] Loss: 0.0082\n",
      "Batch [3680/5778] Loss: 0.0122\n",
      "Batch [3690/5778] Loss: 0.0162\n",
      "Batch [3700/5778] Loss: 0.0044\n",
      "Batch [3710/5778] Loss: 0.0110\n",
      "Batch [3720/5778] Loss: 0.0045\n",
      "Batch [3730/5778] Loss: 0.0228\n",
      "Batch [3740/5778] Loss: 0.0100\n",
      "Batch [3750/5778] Loss: 0.0108\n",
      "Batch [3760/5778] Loss: 0.0024\n",
      "Batch [3770/5778] Loss: 0.0101\n",
      "Batch [3780/5778] Loss: 0.0070\n",
      "Batch [3790/5778] Loss: 0.0107\n",
      "Batch [3800/5778] Loss: 0.0170\n",
      "Batch [3810/5778] Loss: 0.0051\n",
      "Batch [3820/5778] Loss: 0.0150\n",
      "Batch [3830/5778] Loss: 0.0032\n",
      "Batch [3840/5778] Loss: 0.0067\n",
      "Batch [3850/5778] Loss: 0.0090\n",
      "Batch [3860/5778] Loss: 0.0075\n",
      "Batch [3870/5778] Loss: 0.0038\n",
      "Batch [3880/5778] Loss: 0.0100\n",
      "Batch [3890/5778] Loss: 0.0052\n",
      "Batch [3900/5778] Loss: 0.0155\n",
      "Batch [3910/5778] Loss: 0.0093\n",
      "Batch [3920/5778] Loss: 0.0115\n",
      "Batch [3930/5778] Loss: 0.0084\n",
      "Batch [3940/5778] Loss: 0.0105\n",
      "Batch [3950/5778] Loss: 0.0072\n",
      "Batch [3960/5778] Loss: 0.0086\n",
      "Batch [3970/5778] Loss: 0.0121\n",
      "Batch [3980/5778] Loss: 0.0132\n",
      "Batch [3990/5778] Loss: 0.0039\n",
      "Batch [4000/5778] Loss: 0.0084\n",
      "Batch [4010/5778] Loss: 0.0118\n",
      "Batch [4020/5778] Loss: 0.0240\n",
      "Batch [4030/5778] Loss: 0.0109\n",
      "Batch [4040/5778] Loss: 0.0079\n",
      "Batch [4050/5778] Loss: 0.0039\n",
      "Batch [4060/5778] Loss: 0.0117\n",
      "Batch [4070/5778] Loss: 0.0086\n",
      "Batch [4080/5778] Loss: 0.0091\n",
      "Batch [4090/5778] Loss: 0.0092\n",
      "Batch [4100/5778] Loss: 0.0182\n",
      "Batch [4110/5778] Loss: 0.0061\n",
      "Batch [4120/5778] Loss: 0.0102\n",
      "Batch [4130/5778] Loss: 0.0171\n",
      "Batch [4140/5778] Loss: 0.0070\n",
      "Batch [4150/5778] Loss: 0.0050\n",
      "Batch [4160/5778] Loss: 0.0075\n",
      "Batch [4170/5778] Loss: 0.0083\n",
      "Batch [4180/5778] Loss: 0.0082\n",
      "Batch [4190/5778] Loss: 0.0121\n",
      "Batch [4200/5778] Loss: 0.0102\n",
      "Batch [4210/5778] Loss: 0.0087\n",
      "Batch [4220/5778] Loss: 0.0010\n",
      "Batch [4230/5778] Loss: 0.0145\n",
      "Batch [4240/5778] Loss: 0.0060\n",
      "Batch [4250/5778] Loss: 0.0115\n",
      "Batch [4260/5778] Loss: 0.0098\n",
      "Batch [4270/5778] Loss: 0.0072\n",
      "Batch [4280/5778] Loss: 0.0087\n",
      "Batch [4290/5778] Loss: 0.0098\n",
      "Batch [4300/5778] Loss: 0.0113\n",
      "Batch [4310/5778] Loss: 0.0023\n",
      "Batch [4320/5778] Loss: 0.0093\n",
      "Batch [4330/5778] Loss: 0.0084\n",
      "Batch [4340/5778] Loss: 0.0057\n",
      "Batch [4350/5778] Loss: 0.0073\n",
      "Batch [4360/5778] Loss: 0.0108\n",
      "Batch [4370/5778] Loss: 0.0101\n",
      "Batch [4380/5778] Loss: 0.0135\n",
      "Batch [4390/5778] Loss: 0.0061\n",
      "Batch [4400/5778] Loss: 0.0100\n",
      "Batch [4410/5778] Loss: 0.0057\n",
      "Batch [4420/5778] Loss: 0.0097\n",
      "Batch [4430/5778] Loss: 0.0158\n",
      "Batch [4440/5778] Loss: 0.0045\n",
      "Batch [4450/5778] Loss: 0.0055\n",
      "Batch [4460/5778] Loss: 0.0099\n",
      "Batch [4470/5778] Loss: 0.0106\n",
      "Batch [4480/5778] Loss: 0.0103\n",
      "Batch [4490/5778] Loss: 0.0131\n",
      "Batch [4500/5778] Loss: 0.0041\n",
      "Batch [4510/5778] Loss: 0.0173\n",
      "Batch [4520/5778] Loss: 0.0076\n",
      "Batch [4530/5778] Loss: 0.0075\n",
      "Batch [4540/5778] Loss: 0.0068\n",
      "Batch [4550/5778] Loss: 0.0062\n",
      "Batch [4560/5778] Loss: 0.0116\n",
      "Batch [4570/5778] Loss: 0.0068\n",
      "Batch [4580/5778] Loss: 0.0090\n",
      "Batch [4590/5778] Loss: 0.0121\n",
      "Batch [4600/5778] Loss: 0.0103\n",
      "Batch [4610/5778] Loss: 0.0074\n",
      "Batch [4620/5778] Loss: 0.0122\n",
      "Batch [4630/5778] Loss: 0.0140\n",
      "Batch [4640/5778] Loss: 0.0080\n",
      "Batch [4650/5778] Loss: 0.0177\n",
      "Batch [4660/5778] Loss: 0.0162\n",
      "Batch [4670/5778] Loss: 0.0055\n",
      "Batch [4680/5778] Loss: 0.0063\n",
      "Batch [4690/5778] Loss: 0.0093\n",
      "Batch [4700/5778] Loss: 0.0121\n",
      "Batch [4710/5778] Loss: 0.0053\n",
      "Batch [4720/5778] Loss: 0.0059\n",
      "Batch [4730/5778] Loss: 0.0134\n",
      "Batch [4740/5778] Loss: 0.0039\n",
      "Batch [4750/5778] Loss: 0.0100\n",
      "Batch [4760/5778] Loss: 0.0145\n",
      "Batch [4770/5778] Loss: 0.0095\n",
      "Batch [4780/5778] Loss: 0.0135\n",
      "Batch [4790/5778] Loss: 0.0113\n",
      "Batch [4800/5778] Loss: 0.0062\n",
      "Batch [4810/5778] Loss: 0.0098\n",
      "Batch [4820/5778] Loss: 0.0057\n",
      "Batch [4830/5778] Loss: 0.0059\n",
      "Batch [4840/5778] Loss: 0.0017\n",
      "Batch [4850/5778] Loss: 0.0099\n",
      "Batch [4860/5778] Loss: 0.0020\n",
      "Batch [4870/5778] Loss: 0.0231\n",
      "Batch [4880/5778] Loss: 0.0147\n",
      "Batch [4890/5778] Loss: 0.0050\n",
      "Batch [4900/5778] Loss: 0.0034\n",
      "Batch [4910/5778] Loss: 0.0139\n",
      "Batch [4920/5778] Loss: 0.0085\n",
      "Batch [4930/5778] Loss: 0.0119\n",
      "Batch [4940/5778] Loss: 0.0061\n",
      "Batch [4950/5778] Loss: 0.0027\n",
      "Batch [4960/5778] Loss: 0.0201\n",
      "Batch [4970/5778] Loss: 0.0162\n",
      "Batch [4980/5778] Loss: 0.0075\n",
      "Batch [4990/5778] Loss: 0.0067\n",
      "Batch [5000/5778] Loss: 0.0111\n",
      "Batch [5010/5778] Loss: 0.0076\n",
      "Batch [5020/5778] Loss: 0.0143\n",
      "Batch [5030/5778] Loss: 0.0096\n",
      "Batch [5040/5778] Loss: 0.0257\n",
      "Batch [5050/5778] Loss: 0.0036\n",
      "Batch [5060/5778] Loss: 0.0050\n",
      "Batch [5070/5778] Loss: 0.0129\n",
      "Batch [5080/5778] Loss: 0.0162\n",
      "Batch [5090/5778] Loss: 0.0084\n",
      "Batch [5100/5778] Loss: 0.0057\n",
      "Batch [5110/5778] Loss: 0.0063\n",
      "Batch [5120/5778] Loss: 0.0125\n",
      "Batch [5130/5778] Loss: 0.0124\n",
      "Batch [5140/5778] Loss: 0.0167\n",
      "Batch [5150/5778] Loss: 0.0081\n",
      "Batch [5160/5778] Loss: 0.0111\n",
      "Batch [5170/5778] Loss: 0.0175\n",
      "Batch [5180/5778] Loss: 0.0054\n",
      "Batch [5190/5778] Loss: 0.0091\n",
      "Batch [5200/5778] Loss: 0.0054\n",
      "Batch [5210/5778] Loss: 0.0032\n",
      "Batch [5220/5778] Loss: 0.0121\n",
      "Batch [5230/5778] Loss: 0.0057\n",
      "Batch [5240/5778] Loss: 0.0119\n",
      "Batch [5250/5778] Loss: 0.0159\n",
      "Batch [5260/5778] Loss: 0.0072\n",
      "Batch [5270/5778] Loss: 0.0091\n",
      "Batch [5280/5778] Loss: 0.0062\n",
      "Batch [5290/5778] Loss: 0.0120\n",
      "Batch [5300/5778] Loss: 0.0045\n",
      "Batch [5310/5778] Loss: 0.0155\n",
      "Batch [5320/5778] Loss: 0.0052\n",
      "Batch [5330/5778] Loss: 0.0090\n",
      "Batch [5340/5778] Loss: 0.0092\n",
      "Batch [5350/5778] Loss: 0.0187\n",
      "Batch [5360/5778] Loss: 0.0082\n",
      "Batch [5370/5778] Loss: 0.0058\n",
      "Batch [5380/5778] Loss: 0.0075\n",
      "Batch [5390/5778] Loss: 0.0124\n",
      "Batch [5400/5778] Loss: 0.0169\n",
      "Batch [5410/5778] Loss: 0.0037\n",
      "Batch [5420/5778] Loss: 0.0092\n",
      "Batch [5430/5778] Loss: 0.0155\n",
      "Batch [5440/5778] Loss: 0.0131\n",
      "Batch [5450/5778] Loss: 0.0060\n",
      "Batch [5460/5778] Loss: 0.0095\n",
      "Batch [5470/5778] Loss: 0.0094\n",
      "Batch [5480/5778] Loss: 0.0132\n",
      "Batch [5490/5778] Loss: 0.0105\n",
      "Batch [5500/5778] Loss: 0.0066\n",
      "Batch [5510/5778] Loss: 0.0088\n",
      "Batch [5520/5778] Loss: 0.0047\n",
      "Batch [5530/5778] Loss: 0.0071\n",
      "Batch [5540/5778] Loss: 0.0067\n",
      "Batch [5550/5778] Loss: 0.0133\n",
      "Batch [5560/5778] Loss: 0.0084\n",
      "Batch [5570/5778] Loss: 0.0070\n",
      "Batch [5580/5778] Loss: 0.0048\n",
      "Batch [5590/5778] Loss: 0.0045\n",
      "Batch [5600/5778] Loss: 0.0049\n",
      "Batch [5610/5778] Loss: 0.0109\n",
      "Batch [5620/5778] Loss: 0.0079\n",
      "Batch [5630/5778] Loss: 0.0044\n",
      "Batch [5640/5778] Loss: 0.0075\n",
      "Batch [5650/5778] Loss: 0.0067\n",
      "Batch [5660/5778] Loss: 0.0077\n",
      "Batch [5670/5778] Loss: 0.0132\n",
      "Batch [5680/5778] Loss: 0.0129\n",
      "Batch [5690/5778] Loss: 0.0180\n",
      "Batch [5700/5778] Loss: 0.0081\n",
      "Batch [5710/5778] Loss: 0.0027\n",
      "Batch [5720/5778] Loss: 0.0235\n",
      "Batch [5730/5778] Loss: 0.0086\n",
      "Batch [5740/5778] Loss: 0.0059\n",
      "Batch [5750/5778] Loss: 0.0100\n",
      "Batch [5760/5778] Loss: 0.0104\n",
      "Batch [5770/5778] Loss: 0.0078\n",
      "Epoch [4/5] Average Loss: 0.0098\n",
      "Epoch [4/5] Validation Loss: 0.0093 Accuracy: 99.64%\n",
      "Epoch [5/5]\n",
      "Batch [0/5778] Loss: 0.0049\n",
      "Batch [10/5778] Loss: 0.0062\n",
      "Batch [20/5778] Loss: 0.0091\n",
      "Batch [30/5778] Loss: 0.0138\n",
      "Batch [40/5778] Loss: 0.0185\n",
      "Batch [50/5778] Loss: 0.0051\n",
      "Batch [60/5778] Loss: 0.0107\n",
      "Batch [70/5778] Loss: 0.0104\n",
      "Batch [80/5778] Loss: 0.0082\n",
      "Batch [90/5778] Loss: 0.0067\n",
      "Batch [100/5778] Loss: 0.0118\n",
      "Batch [110/5778] Loss: 0.0032\n",
      "Batch [120/5778] Loss: 0.0138\n",
      "Batch [130/5778] Loss: 0.0231\n",
      "Batch [140/5778] Loss: 0.0092\n",
      "Batch [150/5778] Loss: 0.0100\n",
      "Batch [160/5778] Loss: 0.0070\n",
      "Batch [170/5778] Loss: 0.0051\n",
      "Batch [180/5778] Loss: 0.0127\n",
      "Batch [190/5778] Loss: 0.0134\n",
      "Batch [200/5778] Loss: 0.0080\n",
      "Batch [210/5778] Loss: 0.0068\n",
      "Batch [220/5778] Loss: 0.0045\n",
      "Batch [230/5778] Loss: 0.0067\n",
      "Batch [240/5778] Loss: 0.0057\n",
      "Batch [250/5778] Loss: 0.0102\n",
      "Batch [260/5778] Loss: 0.0064\n",
      "Batch [270/5778] Loss: 0.0047\n",
      "Batch [280/5778] Loss: 0.0154\n",
      "Batch [290/5778] Loss: 0.0113\n",
      "Batch [300/5778] Loss: 0.0042\n",
      "Batch [310/5778] Loss: 0.0067\n",
      "Batch [320/5778] Loss: 0.0063\n",
      "Batch [330/5778] Loss: 0.0069\n",
      "Batch [340/5778] Loss: 0.0098\n",
      "Batch [350/5778] Loss: 0.0235\n",
      "Batch [360/5778] Loss: 0.0174\n",
      "Batch [370/5778] Loss: 0.0077\n",
      "Batch [380/5778] Loss: 0.0110\n",
      "Batch [390/5778] Loss: 0.0120\n",
      "Batch [400/5778] Loss: 0.0091\n",
      "Batch [410/5778] Loss: 0.0075\n",
      "Batch [420/5778] Loss: 0.0060\n",
      "Batch [430/5778] Loss: 0.0029\n",
      "Batch [440/5778] Loss: 0.0066\n",
      "Batch [450/5778] Loss: 0.0058\n",
      "Batch [460/5778] Loss: 0.0137\n",
      "Batch [470/5778] Loss: 0.0089\n",
      "Batch [480/5778] Loss: 0.0060\n",
      "Batch [490/5778] Loss: 0.0031\n",
      "Batch [500/5778] Loss: 0.0045\n",
      "Batch [510/5778] Loss: 0.0110\n",
      "Batch [520/5778] Loss: 0.0040\n",
      "Batch [530/5778] Loss: 0.0087\n",
      "Batch [540/5778] Loss: 0.0075\n",
      "Batch [550/5778] Loss: 0.0049\n",
      "Batch [560/5778] Loss: 0.0104\n",
      "Batch [570/5778] Loss: 0.0045\n",
      "Batch [580/5778] Loss: 0.0115\n",
      "Batch [590/5778] Loss: 0.0036\n",
      "Batch [600/5778] Loss: 0.0070\n",
      "Batch [610/5778] Loss: 0.0133\n",
      "Batch [620/5778] Loss: 0.0129\n",
      "Batch [630/5778] Loss: 0.0112\n",
      "Batch [640/5778] Loss: 0.0122\n",
      "Batch [650/5778] Loss: 0.0033\n",
      "Batch [660/5778] Loss: 0.0114\n",
      "Batch [670/5778] Loss: 0.0042\n",
      "Batch [680/5778] Loss: 0.0103\n",
      "Batch [690/5778] Loss: 0.0094\n",
      "Batch [700/5778] Loss: 0.0037\n",
      "Batch [710/5778] Loss: 0.0033\n",
      "Batch [720/5778] Loss: 0.0062\n",
      "Batch [730/5778] Loss: 0.0147\n",
      "Batch [740/5778] Loss: 0.0081\n",
      "Batch [750/5778] Loss: 0.0083\n",
      "Batch [760/5778] Loss: 0.0046\n",
      "Batch [770/5778] Loss: 0.0020\n",
      "Batch [780/5778] Loss: 0.0067\n",
      "Batch [790/5778] Loss: 0.0048\n",
      "Batch [800/5778] Loss: 0.0142\n",
      "Batch [810/5778] Loss: 0.0114\n",
      "Batch [820/5778] Loss: 0.0111\n",
      "Batch [830/5778] Loss: 0.0039\n",
      "Batch [840/5778] Loss: 0.0042\n",
      "Batch [850/5778] Loss: 0.0078\n",
      "Batch [860/5778] Loss: 0.0094\n",
      "Batch [870/5778] Loss: 0.0017\n",
      "Batch [880/5778] Loss: 0.0059\n",
      "Batch [890/5778] Loss: 0.0091\n",
      "Batch [900/5778] Loss: 0.0069\n",
      "Batch [910/5778] Loss: 0.0055\n",
      "Batch [920/5778] Loss: 0.0091\n",
      "Batch [930/5778] Loss: 0.0121\n",
      "Batch [940/5778] Loss: 0.0093\n",
      "Batch [950/5778] Loss: 0.0162\n",
      "Batch [960/5778] Loss: 0.0084\n",
      "Batch [970/5778] Loss: 0.0040\n",
      "Batch [980/5778] Loss: 0.0092\n",
      "Batch [990/5778] Loss: 0.0039\n",
      "Batch [1000/5778] Loss: 0.0144\n",
      "Batch [1010/5778] Loss: 0.0039\n",
      "Batch [1020/5778] Loss: 0.0077\n",
      "Batch [1030/5778] Loss: 0.0079\n",
      "Batch [1040/5778] Loss: 0.0041\n",
      "Batch [1050/5778] Loss: 0.0066\n",
      "Batch [1060/5778] Loss: 0.0107\n",
      "Batch [1070/5778] Loss: 0.0100\n",
      "Batch [1080/5778] Loss: 0.0060\n",
      "Batch [1090/5778] Loss: 0.0088\n",
      "Batch [1100/5778] Loss: 0.0081\n",
      "Batch [1110/5778] Loss: 0.0072\n",
      "Batch [1120/5778] Loss: 0.0090\n",
      "Batch [1130/5778] Loss: 0.0095\n",
      "Batch [1140/5778] Loss: 0.0081\n",
      "Batch [1150/5778] Loss: 0.0171\n",
      "Batch [1160/5778] Loss: 0.0183\n",
      "Batch [1170/5778] Loss: 0.0086\n",
      "Batch [1180/5778] Loss: 0.0179\n",
      "Batch [1190/5778] Loss: 0.0048\n",
      "Batch [1200/5778] Loss: 0.0108\n",
      "Batch [1210/5778] Loss: 0.0087\n",
      "Batch [1220/5778] Loss: 0.0099\n",
      "Batch [1230/5778] Loss: 0.0087\n",
      "Batch [1240/5778] Loss: 0.0106\n",
      "Batch [1250/5778] Loss: 0.0065\n",
      "Batch [1260/5778] Loss: 0.0041\n",
      "Batch [1270/5778] Loss: 0.0071\n",
      "Batch [1280/5778] Loss: 0.0140\n",
      "Batch [1290/5778] Loss: 0.0027\n",
      "Batch [1300/5778] Loss: 0.0074\n",
      "Batch [1310/5778] Loss: 0.0106\n",
      "Batch [1320/5778] Loss: 0.0057\n",
      "Batch [1330/5778] Loss: 0.0097\n",
      "Batch [1340/5778] Loss: 0.0116\n",
      "Batch [1350/5778] Loss: 0.0097\n",
      "Batch [1360/5778] Loss: 0.0128\n",
      "Batch [1370/5778] Loss: 0.0210\n",
      "Batch [1380/5778] Loss: 0.0057\n",
      "Batch [1390/5778] Loss: 0.0101\n",
      "Batch [1400/5778] Loss: 0.0057\n",
      "Batch [1410/5778] Loss: 0.0096\n",
      "Batch [1420/5778] Loss: 0.0109\n",
      "Batch [1430/5778] Loss: 0.0123\n",
      "Batch [1440/5778] Loss: 0.0120\n",
      "Batch [1450/5778] Loss: 0.0048\n",
      "Batch [1460/5778] Loss: 0.0104\n",
      "Batch [1470/5778] Loss: 0.0025\n",
      "Batch [1480/5778] Loss: 0.0078\n",
      "Batch [1490/5778] Loss: 0.0081\n",
      "Batch [1500/5778] Loss: 0.0044\n",
      "Batch [1510/5778] Loss: 0.0121\n",
      "Batch [1520/5778] Loss: 0.0155\n",
      "Batch [1530/5778] Loss: 0.0106\n",
      "Batch [1540/5778] Loss: 0.0065\n",
      "Batch [1550/5778] Loss: 0.0095\n",
      "Batch [1560/5778] Loss: 0.0126\n",
      "Batch [1570/5778] Loss: 0.0116\n",
      "Batch [1580/5778] Loss: 0.0093\n",
      "Batch [1590/5778] Loss: 0.0103\n",
      "Batch [1600/5778] Loss: 0.0040\n",
      "Batch [1610/5778] Loss: 0.0075\n",
      "Batch [1620/5778] Loss: 0.0049\n",
      "Batch [1630/5778] Loss: 0.0100\n",
      "Batch [1640/5778] Loss: 0.0051\n",
      "Batch [1650/5778] Loss: 0.0123\n",
      "Batch [1660/5778] Loss: 0.0072\n",
      "Batch [1670/5778] Loss: 0.0093\n",
      "Batch [1680/5778] Loss: 0.0049\n",
      "Batch [1690/5778] Loss: 0.0039\n",
      "Batch [1700/5778] Loss: 0.0071\n",
      "Batch [1710/5778] Loss: 0.0129\n",
      "Batch [1720/5778] Loss: 0.0097\n",
      "Batch [1730/5778] Loss: 0.0107\n",
      "Batch [1740/5778] Loss: 0.0188\n",
      "Batch [1750/5778] Loss: 0.0091\n",
      "Batch [1760/5778] Loss: 0.0125\n",
      "Batch [1770/5778] Loss: 0.0076\n",
      "Batch [1780/5778] Loss: 0.0095\n",
      "Batch [1790/5778] Loss: 0.0057\n",
      "Batch [1800/5778] Loss: 0.0053\n",
      "Batch [1810/5778] Loss: 0.0093\n",
      "Batch [1820/5778] Loss: 0.0115\n",
      "Batch [1830/5778] Loss: 0.0164\n",
      "Batch [1840/5778] Loss: 0.0043\n",
      "Batch [1850/5778] Loss: 0.0153\n",
      "Batch [1860/5778] Loss: 0.0057\n",
      "Batch [1870/5778] Loss: 0.0052\n",
      "Batch [1880/5778] Loss: 0.0075\n",
      "Batch [1890/5778] Loss: 0.0099\n",
      "Batch [1900/5778] Loss: 0.0064\n",
      "Batch [1910/5778] Loss: 0.0013\n",
      "Batch [1920/5778] Loss: 0.0032\n",
      "Batch [1930/5778] Loss: 0.0101\n",
      "Batch [1940/5778] Loss: 0.0072\n",
      "Batch [1950/5778] Loss: 0.0049\n",
      "Batch [1960/5778] Loss: 0.0184\n",
      "Batch [1970/5778] Loss: 0.0066\n",
      "Batch [1980/5778] Loss: 0.0069\n",
      "Batch [1990/5778] Loss: 0.0232\n",
      "Batch [2000/5778] Loss: 0.0067\n",
      "Batch [2010/5778] Loss: 0.0091\n",
      "Batch [2020/5778] Loss: 0.0061\n",
      "Batch [2030/5778] Loss: 0.0076\n",
      "Batch [2040/5778] Loss: 0.0055\n",
      "Batch [2050/5778] Loss: 0.0064\n",
      "Batch [2060/5778] Loss: 0.0098\n",
      "Batch [2070/5778] Loss: 0.0085\n",
      "Batch [2080/5778] Loss: 0.0044\n",
      "Batch [2090/5778] Loss: 0.0161\n",
      "Batch [2100/5778] Loss: 0.0130\n",
      "Batch [2110/5778] Loss: 0.0056\n",
      "Batch [2120/5778] Loss: 0.0062\n",
      "Batch [2130/5778] Loss: 0.0090\n",
      "Batch [2140/5778] Loss: 0.0162\n",
      "Batch [2150/5778] Loss: 0.0101\n",
      "Batch [2160/5778] Loss: 0.0081\n",
      "Batch [2170/5778] Loss: 0.0088\n",
      "Batch [2180/5778] Loss: 0.0052\n",
      "Batch [2190/5778] Loss: 0.0075\n",
      "Batch [2200/5778] Loss: 0.0081\n",
      "Batch [2210/5778] Loss: 0.0072\n",
      "Batch [2220/5778] Loss: 0.0070\n",
      "Batch [2230/5778] Loss: 0.0152\n",
      "Batch [2240/5778] Loss: 0.0117\n",
      "Batch [2250/5778] Loss: 0.0117\n",
      "Batch [2260/5778] Loss: 0.0170\n",
      "Batch [2270/5778] Loss: 0.0094\n",
      "Batch [2280/5778] Loss: 0.0126\n",
      "Batch [2290/5778] Loss: 0.0073\n",
      "Batch [2300/5778] Loss: 0.0082\n",
      "Batch [2310/5778] Loss: 0.0048\n",
      "Batch [2320/5778] Loss: 0.0058\n",
      "Batch [2330/5778] Loss: 0.0040\n",
      "Batch [2340/5778] Loss: 0.0024\n",
      "Batch [2350/5778] Loss: 0.0060\n",
      "Batch [2360/5778] Loss: 0.0044\n",
      "Batch [2370/5778] Loss: 0.0145\n",
      "Batch [2380/5778] Loss: 0.0051\n",
      "Batch [2390/5778] Loss: 0.0033\n",
      "Batch [2400/5778] Loss: 0.0107\n",
      "Batch [2410/5778] Loss: 0.0052\n",
      "Batch [2420/5778] Loss: 0.0068\n",
      "Batch [2430/5778] Loss: 0.0108\n",
      "Batch [2440/5778] Loss: 0.0057\n",
      "Batch [2450/5778] Loss: 0.0111\n",
      "Batch [2460/5778] Loss: 0.0047\n",
      "Batch [2470/5778] Loss: 0.0105\n",
      "Batch [2480/5778] Loss: 0.0110\n",
      "Batch [2490/5778] Loss: 0.0064\n",
      "Batch [2500/5778] Loss: 0.0153\n",
      "Batch [2510/5778] Loss: 0.0249\n",
      "Batch [2520/5778] Loss: 0.0073\n",
      "Batch [2530/5778] Loss: 0.0121\n",
      "Batch [2540/5778] Loss: 0.0097\n",
      "Batch [2550/5778] Loss: 0.0043\n",
      "Batch [2560/5778] Loss: 0.0056\n",
      "Batch [2570/5778] Loss: 0.0077\n",
      "Batch [2580/5778] Loss: 0.0100\n",
      "Batch [2590/5778] Loss: 0.0048\n",
      "Batch [2600/5778] Loss: 0.0103\n",
      "Batch [2610/5778] Loss: 0.0182\n",
      "Batch [2620/5778] Loss: 0.0071\n",
      "Batch [2630/5778] Loss: 0.0075\n",
      "Batch [2640/5778] Loss: 0.0133\n",
      "Batch [2650/5778] Loss: 0.0090\n",
      "Batch [2660/5778] Loss: 0.0127\n",
      "Batch [2670/5778] Loss: 0.0081\n",
      "Batch [2680/5778] Loss: 0.0164\n",
      "Batch [2690/5778] Loss: 0.0055\n",
      "Batch [2700/5778] Loss: 0.0099\n",
      "Batch [2710/5778] Loss: 0.0056\n",
      "Batch [2720/5778] Loss: 0.0079\n",
      "Batch [2730/5778] Loss: 0.0100\n",
      "Batch [2740/5778] Loss: 0.0145\n",
      "Batch [2750/5778] Loss: 0.0027\n",
      "Batch [2760/5778] Loss: 0.0113\n",
      "Batch [2770/5778] Loss: 0.0097\n",
      "Batch [2780/5778] Loss: 0.0141\n",
      "Batch [2790/5778] Loss: 0.0070\n",
      "Batch [2800/5778] Loss: 0.0130\n",
      "Batch [2810/5778] Loss: 0.0018\n",
      "Batch [2820/5778] Loss: 0.0074\n",
      "Batch [2830/5778] Loss: 0.0106\n",
      "Batch [2840/5778] Loss: 0.0095\n",
      "Batch [2850/5778] Loss: 0.0114\n",
      "Batch [2860/5778] Loss: 0.0061\n",
      "Batch [2870/5778] Loss: 0.0098\n",
      "Batch [2880/5778] Loss: 0.0117\n",
      "Batch [2890/5778] Loss: 0.0137\n",
      "Batch [2900/5778] Loss: 0.0069\n",
      "Batch [2910/5778] Loss: 0.0150\n",
      "Batch [2920/5778] Loss: 0.0094\n",
      "Batch [2930/5778] Loss: 0.0051\n",
      "Batch [2940/5778] Loss: 0.0082\n",
      "Batch [2950/5778] Loss: 0.0192\n",
      "Batch [2960/5778] Loss: 0.0112\n",
      "Batch [2970/5778] Loss: 0.0066\n",
      "Batch [2980/5778] Loss: 0.0084\n",
      "Batch [2990/5778] Loss: 0.0099\n",
      "Batch [3000/5778] Loss: 0.0097\n",
      "Batch [3010/5778] Loss: 0.0143\n",
      "Batch [3020/5778] Loss: 0.0013\n",
      "Batch [3030/5778] Loss: 0.0088\n",
      "Batch [3040/5778] Loss: 0.0111\n",
      "Batch [3050/5778] Loss: 0.0058\n",
      "Batch [3060/5778] Loss: 0.0035\n",
      "Batch [3070/5778] Loss: 0.0052\n",
      "Batch [3080/5778] Loss: 0.0051\n",
      "Batch [3090/5778] Loss: 0.0052\n",
      "Batch [3100/5778] Loss: 0.0167\n",
      "Batch [3110/5778] Loss: 0.0127\n",
      "Batch [3120/5778] Loss: 0.0106\n",
      "Batch [3130/5778] Loss: 0.0049\n",
      "Batch [3140/5778] Loss: 0.0142\n",
      "Batch [3150/5778] Loss: 0.0097\n",
      "Batch [3160/5778] Loss: 0.0061\n",
      "Batch [3170/5778] Loss: 0.0057\n",
      "Batch [3180/5778] Loss: 0.0091\n",
      "Batch [3190/5778] Loss: 0.0097\n",
      "Batch [3200/5778] Loss: 0.0086\n",
      "Batch [3210/5778] Loss: 0.0061\n",
      "Batch [3220/5778] Loss: 0.0136\n",
      "Batch [3230/5778] Loss: 0.0084\n",
      "Batch [3240/5778] Loss: 0.0095\n",
      "Batch [3250/5778] Loss: 0.0099\n",
      "Batch [3260/5778] Loss: 0.0032\n",
      "Batch [3270/5778] Loss: 0.0094\n",
      "Batch [3280/5778] Loss: 0.0104\n",
      "Batch [3290/5778] Loss: 0.0096\n",
      "Batch [3300/5778] Loss: 0.0071\n",
      "Batch [3310/5778] Loss: 0.0098\n",
      "Batch [3320/5778] Loss: 0.0137\n",
      "Batch [3330/5778] Loss: 0.0108\n",
      "Batch [3340/5778] Loss: 0.0050\n",
      "Batch [3350/5778] Loss: 0.0171\n",
      "Batch [3360/5778] Loss: 0.0049\n",
      "Batch [3370/5778] Loss: 0.0050\n",
      "Batch [3380/5778] Loss: 0.0058\n",
      "Batch [3390/5778] Loss: 0.0153\n",
      "Batch [3400/5778] Loss: 0.0077\n",
      "Batch [3410/5778] Loss: 0.0108\n",
      "Batch [3420/5778] Loss: 0.0054\n",
      "Batch [3430/5778] Loss: 0.0108\n",
      "Batch [3440/5778] Loss: 0.0126\n",
      "Batch [3450/5778] Loss: 0.0080\n",
      "Batch [3460/5778] Loss: 0.0102\n",
      "Batch [3470/5778] Loss: 0.0080\n",
      "Batch [3480/5778] Loss: 0.0098\n",
      "Batch [3490/5778] Loss: 0.0201\n",
      "Batch [3500/5778] Loss: 0.0140\n",
      "Batch [3510/5778] Loss: 0.0055\n",
      "Batch [3520/5778] Loss: 0.0097\n",
      "Batch [3530/5778] Loss: 0.0125\n",
      "Batch [3540/5778] Loss: 0.0097\n",
      "Batch [3550/5778] Loss: 0.0067\n",
      "Batch [3560/5778] Loss: 0.0056\n",
      "Batch [3570/5778] Loss: 0.0069\n",
      "Batch [3580/5778] Loss: 0.0085\n",
      "Batch [3590/5778] Loss: 0.0047\n",
      "Batch [3600/5778] Loss: 0.0081\n",
      "Batch [3610/5778] Loss: 0.0086\n",
      "Batch [3620/5778] Loss: 0.0109\n",
      "Batch [3630/5778] Loss: 0.0115\n",
      "Batch [3640/5778] Loss: 0.0031\n",
      "Batch [3650/5778] Loss: 0.0050\n",
      "Batch [3660/5778] Loss: 0.0089\n",
      "Batch [3670/5778] Loss: 0.0017\n",
      "Batch [3680/5778] Loss: 0.0150\n",
      "Batch [3690/5778] Loss: 0.0071\n",
      "Batch [3700/5778] Loss: 0.0067\n",
      "Batch [3710/5778] Loss: 0.0096\n",
      "Batch [3720/5778] Loss: 0.0025\n",
      "Batch [3730/5778] Loss: 0.0116\n",
      "Batch [3740/5778] Loss: 0.0108\n",
      "Batch [3750/5778] Loss: 0.0066\n",
      "Batch [3760/5778] Loss: 0.0204\n",
      "Batch [3770/5778] Loss: 0.0134\n",
      "Batch [3780/5778] Loss: 0.0082\n",
      "Batch [3790/5778] Loss: 0.0075\n",
      "Batch [3800/5778] Loss: 0.0081\n",
      "Batch [3810/5778] Loss: 0.0126\n",
      "Batch [3820/5778] Loss: 0.0118\n",
      "Batch [3830/5778] Loss: 0.0065\n",
      "Batch [3840/5778] Loss: 0.0170\n",
      "Batch [3850/5778] Loss: 0.0115\n",
      "Batch [3860/5778] Loss: 0.0048\n",
      "Batch [3870/5778] Loss: 0.0135\n",
      "Batch [3880/5778] Loss: 0.0098\n",
      "Batch [3890/5778] Loss: 0.0095\n",
      "Batch [3900/5778] Loss: 0.0123\n",
      "Batch [3910/5778] Loss: 0.0078\n",
      "Batch [3920/5778] Loss: 0.0058\n",
      "Batch [3930/5778] Loss: 0.0119\n",
      "Batch [3940/5778] Loss: 0.0126\n",
      "Batch [3950/5778] Loss: 0.0133\n",
      "Batch [3960/5778] Loss: 0.0147\n",
      "Batch [3970/5778] Loss: 0.0080\n",
      "Batch [3980/5778] Loss: 0.0099\n",
      "Batch [3990/5778] Loss: 0.0033\n",
      "Batch [4000/5778] Loss: 0.0068\n",
      "Batch [4010/5778] Loss: 0.0126\n",
      "Batch [4020/5778] Loss: 0.0068\n",
      "Batch [4030/5778] Loss: 0.0076\n",
      "Batch [4040/5778] Loss: 0.0035\n",
      "Batch [4050/5778] Loss: 0.0141\n",
      "Batch [4060/5778] Loss: 0.0201\n",
      "Batch [4070/5778] Loss: 0.0109\n",
      "Batch [4080/5778] Loss: 0.0045\n",
      "Batch [4090/5778] Loss: 0.0077\n",
      "Batch [4100/5778] Loss: 0.0060\n",
      "Batch [4110/5778] Loss: 0.0090\n",
      "Batch [4120/5778] Loss: 0.0032\n",
      "Batch [4130/5778] Loss: 0.0054\n",
      "Batch [4140/5778] Loss: 0.0148\n",
      "Batch [4150/5778] Loss: 0.0069\n",
      "Batch [4160/5778] Loss: 0.0143\n",
      "Batch [4170/5778] Loss: 0.0068\n",
      "Batch [4180/5778] Loss: 0.0095\n",
      "Batch [4190/5778] Loss: 0.0084\n",
      "Batch [4200/5778] Loss: 0.0127\n",
      "Batch [4210/5778] Loss: 0.0038\n",
      "Batch [4220/5778] Loss: 0.0015\n",
      "Batch [4230/5778] Loss: 0.0088\n",
      "Batch [4240/5778] Loss: 0.0078\n",
      "Batch [4250/5778] Loss: 0.0087\n",
      "Batch [4260/5778] Loss: 0.0043\n",
      "Batch [4270/5778] Loss: 0.0089\n",
      "Batch [4280/5778] Loss: 0.0142\n",
      "Batch [4290/5778] Loss: 0.0163\n",
      "Batch [4300/5778] Loss: 0.0112\n",
      "Batch [4310/5778] Loss: 0.0096\n",
      "Batch [4320/5778] Loss: 0.0076\n",
      "Batch [4330/5778] Loss: 0.0166\n",
      "Batch [4340/5778] Loss: 0.0083\n",
      "Batch [4350/5778] Loss: 0.0153\n",
      "Batch [4360/5778] Loss: 0.0071\n",
      "Batch [4370/5778] Loss: 0.0023\n",
      "Batch [4380/5778] Loss: 0.0120\n",
      "Batch [4390/5778] Loss: 0.0063\n",
      "Batch [4400/5778] Loss: 0.0059\n",
      "Batch [4410/5778] Loss: 0.0186\n",
      "Batch [4420/5778] Loss: 0.0080\n",
      "Batch [4430/5778] Loss: 0.0062\n",
      "Batch [4440/5778] Loss: 0.0132\n",
      "Batch [4450/5778] Loss: 0.0121\n",
      "Batch [4460/5778] Loss: 0.0099\n",
      "Batch [4470/5778] Loss: 0.0060\n",
      "Batch [4480/5778] Loss: 0.0045\n",
      "Batch [4490/5778] Loss: 0.0093\n",
      "Batch [4500/5778] Loss: 0.0089\n",
      "Batch [4510/5778] Loss: 0.0107\n",
      "Batch [4520/5778] Loss: 0.0072\n",
      "Batch [4530/5778] Loss: 0.0130\n",
      "Batch [4540/5778] Loss: 0.0193\n",
      "Batch [4550/5778] Loss: 0.0044\n",
      "Batch [4560/5778] Loss: 0.0073\n",
      "Batch [4570/5778] Loss: 0.0084\n",
      "Batch [4580/5778] Loss: 0.0152\n",
      "Batch [4590/5778] Loss: 0.0136\n",
      "Batch [4600/5778] Loss: 0.0036\n",
      "Batch [4610/5778] Loss: 0.0094\n",
      "Batch [4620/5778] Loss: 0.0073\n",
      "Batch [4630/5778] Loss: 0.0112\n",
      "Batch [4640/5778] Loss: 0.0125\n",
      "Batch [4650/5778] Loss: 0.0121\n",
      "Batch [4660/5778] Loss: 0.0108\n",
      "Batch [4670/5778] Loss: 0.0129\n",
      "Batch [4680/5778] Loss: 0.0094\n",
      "Batch [4690/5778] Loss: 0.0051\n",
      "Batch [4700/5778] Loss: 0.0059\n",
      "Batch [4710/5778] Loss: 0.0112\n",
      "Batch [4720/5778] Loss: 0.0074\n",
      "Batch [4730/5778] Loss: 0.0125\n",
      "Batch [4740/5778] Loss: 0.0178\n",
      "Batch [4750/5778] Loss: 0.0078\n",
      "Batch [4760/5778] Loss: 0.0052\n",
      "Batch [4770/5778] Loss: 0.0171\n",
      "Batch [4780/5778] Loss: 0.0093\n",
      "Batch [4790/5778] Loss: 0.0046\n",
      "Batch [4800/5778] Loss: 0.0108\n",
      "Batch [4810/5778] Loss: 0.0082\n",
      "Batch [4820/5778] Loss: 0.0048\n",
      "Batch [4830/5778] Loss: 0.0073\n",
      "Batch [4840/5778] Loss: 0.0053\n",
      "Batch [4850/5778] Loss: 0.0067\n",
      "Batch [4860/5778] Loss: 0.0131\n",
      "Batch [4870/5778] Loss: 0.0102\n",
      "Batch [4880/5778] Loss: 0.0098\n",
      "Batch [4890/5778] Loss: 0.0148\n",
      "Batch [4900/5778] Loss: 0.0044\n",
      "Batch [4910/5778] Loss: 0.0057\n",
      "Batch [4920/5778] Loss: 0.0075\n",
      "Batch [4930/5778] Loss: 0.0150\n",
      "Batch [4940/5778] Loss: 0.0064\n",
      "Batch [4950/5778] Loss: 0.0093\n",
      "Batch [4960/5778] Loss: 0.0090\n",
      "Batch [4970/5778] Loss: 0.0104\n",
      "Batch [4980/5778] Loss: 0.0040\n",
      "Batch [4990/5778] Loss: 0.0103\n",
      "Batch [5000/5778] Loss: 0.0081\n",
      "Batch [5010/5778] Loss: 0.0030\n",
      "Batch [5020/5778] Loss: 0.0119\n",
      "Batch [5030/5778] Loss: 0.0060\n",
      "Batch [5040/5778] Loss: 0.0045\n",
      "Batch [5050/5778] Loss: 0.0133\n",
      "Batch [5060/5778] Loss: 0.0120\n",
      "Batch [5070/5778] Loss: 0.0150\n",
      "Batch [5080/5778] Loss: 0.0118\n",
      "Batch [5090/5778] Loss: 0.0050\n",
      "Batch [5100/5778] Loss: 0.0141\n",
      "Batch [5110/5778] Loss: 0.0132\n",
      "Batch [5120/5778] Loss: 0.0059\n",
      "Batch [5130/5778] Loss: 0.0038\n",
      "Batch [5140/5778] Loss: 0.0072\n",
      "Batch [5150/5778] Loss: 0.0163\n",
      "Batch [5160/5778] Loss: 0.0072\n",
      "Batch [5170/5778] Loss: 0.0072\n",
      "Batch [5180/5778] Loss: 0.0066\n",
      "Batch [5190/5778] Loss: 0.0085\n",
      "Batch [5200/5778] Loss: 0.0054\n",
      "Batch [5210/5778] Loss: 0.0058\n",
      "Batch [5220/5778] Loss: 0.0069\n",
      "Batch [5230/5778] Loss: 0.0079\n",
      "Batch [5240/5778] Loss: 0.0090\n",
      "Batch [5250/5778] Loss: 0.0038\n",
      "Batch [5260/5778] Loss: 0.0067\n",
      "Batch [5270/5778] Loss: 0.0079\n",
      "Batch [5280/5778] Loss: 0.0104\n",
      "Batch [5290/5778] Loss: 0.0030\n",
      "Batch [5300/5778] Loss: 0.0083\n",
      "Batch [5310/5778] Loss: 0.0047\n",
      "Batch [5320/5778] Loss: 0.0080\n",
      "Batch [5330/5778] Loss: 0.0061\n",
      "Batch [5340/5778] Loss: 0.0113\n",
      "Batch [5350/5778] Loss: 0.0082\n",
      "Batch [5360/5778] Loss: 0.0068\n",
      "Batch [5370/5778] Loss: 0.0052\n",
      "Batch [5380/5778] Loss: 0.0127\n",
      "Batch [5390/5778] Loss: 0.0015\n",
      "Batch [5400/5778] Loss: 0.0111\n",
      "Batch [5410/5778] Loss: 0.0072\n",
      "Batch [5420/5778] Loss: 0.0109\n",
      "Batch [5430/5778] Loss: 0.0037\n",
      "Batch [5440/5778] Loss: 0.0120\n",
      "Batch [5450/5778] Loss: 0.0075\n",
      "Batch [5460/5778] Loss: 0.0068\n",
      "Batch [5470/5778] Loss: 0.0119\n",
      "Batch [5480/5778] Loss: 0.0104\n",
      "Batch [5490/5778] Loss: 0.0346\n",
      "Batch [5500/5778] Loss: 0.0110\n",
      "Batch [5510/5778] Loss: 0.0051\n",
      "Batch [5520/5778] Loss: 0.0074\n",
      "Batch [5530/5778] Loss: 0.0136\n",
      "Batch [5540/5778] Loss: 0.0088\n",
      "Batch [5550/5778] Loss: 0.0078\n",
      "Batch [5560/5778] Loss: 0.0062\n",
      "Batch [5570/5778] Loss: 0.0102\n",
      "Batch [5580/5778] Loss: 0.0064\n",
      "Batch [5590/5778] Loss: 0.0061\n",
      "Batch [5600/5778] Loss: 0.0049\n",
      "Batch [5610/5778] Loss: 0.0154\n",
      "Batch [5620/5778] Loss: 0.0053\n",
      "Batch [5630/5778] Loss: 0.0051\n",
      "Batch [5640/5778] Loss: 0.0079\n",
      "Batch [5650/5778] Loss: 0.0097\n",
      "Batch [5660/5778] Loss: 0.0080\n",
      "Batch [5670/5778] Loss: 0.0174\n",
      "Batch [5680/5778] Loss: 0.0142\n",
      "Batch [5690/5778] Loss: 0.0130\n",
      "Batch [5700/5778] Loss: 0.0043\n",
      "Batch [5710/5778] Loss: 0.0109\n",
      "Batch [5720/5778] Loss: 0.0091\n",
      "Batch [5730/5778] Loss: 0.0073\n",
      "Batch [5740/5778] Loss: 0.0176\n",
      "Batch [5750/5778] Loss: 0.0063\n",
      "Batch [5760/5778] Loss: 0.0103\n",
      "Batch [5770/5778] Loss: 0.0179\n",
      "Epoch [5/5] Average Loss: 0.0089\n",
      "Epoch [5/5] Validation Loss: 0.0094 Accuracy: 99.63%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "num_epochs = 5  # Количество эпох\n",
    "\n",
    "model.train()\n",
    "\n",
    "best_loss = float('inf')\n",
    "consecutive_no_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    total_loss = 0.0  # To keep track of the total loss in the epoch\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        writer.add_scalar('Loss/train', loss, global_step=epoch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print progress every n batches (e.g., every 10 batches)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch [{batch_idx}/{len(train_dataloader)}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    val_accuracy, val_loss = validate_model(model, val_dataloader, criterion)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Validation Loss: {val_loss:.4f} Accuracy: {val_accuracy:.2f}%\")\n",
    "    model.train()  # Switch back to training mode\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        consecutive_no_improvement = 0\n",
    "        # Save the best model if needed\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        consecutive_no_improvement += 1\n",
    "\n",
    "    if consecutive_no_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs without improvement.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T00:29:22.962494400Z",
     "start_time": "2023-09-16T21:16:35.098776700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 99.60%\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели на тестовом наборе данных\n",
    "# Load the best one\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test dataset: {accuracy:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T00:36:23.203492400Z",
     "start_time": "2023-09-17T00:29:22.963491600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lang_model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T00:36:23.240494Z",
     "start_time": "2023-09-17T00:36:23.209494100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('lang_model.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T06:42:59.408117600Z",
     "start_time": "2023-09-17T06:42:59.374118Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predicted Values:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный язык: Русский\n"
     ]
    }
   ],
   "source": [
    "def predict_language(text, model, char_to_index, max_length):\n",
    "    # Шаг 1: Токенизация текста на символы и преобразование символов в индексы\n",
    "    input_indices = [char_to_index.get(char, 0) for char in text if char in char_to_index]\n",
    "\n",
    "    # Шаг 2: Добавление паддинга, если необходимо\n",
    "    if len(input_indices) < max_length:\n",
    "        num_padding = max_length - len(input_indices)\n",
    "        input_indices += [0] * num_padding\n",
    "\n",
    "    # Шаг 3: Преобразование в тензор PyTorch и изменение размерности\n",
    "    input_tensor = torch.tensor(input_indices).view(1, -1).to(device)\n",
    "\n",
    "    # Шаг 4: Передача данных в модель для получения предсказания\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Перевести модель в режим оценки (не тренировки)\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    # Обработка предсказания\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    # Вернуть класс языка на основе предсказания\n",
    "    if predicted_class.item() == 0:\n",
    "        return \"Нет языка\"\n",
    "    elif predicted_class.item() == 1:\n",
    "        return \"Английский\"\n",
    "    elif predicted_class.item() == 2:\n",
    "        return \"Иврит\"\n",
    "    elif predicted_class.item() == 3:\n",
    "        return \"Русский\"\n",
    "\n",
    "# Пример использования\n",
    "input_text = \"привет\"\n",
    "predicted_language = predict_language(input_text, model, char_to_index, max_length)\n",
    "print(\"Предсказанный язык:\", predicted_language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T06:43:29.357163200Z",
     "start_time": "2023-09-17T06:43:29.327166300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
